# Chapter 1: AI Manager - User Guide Overview

> **🚀 QUICK START GUIDE**
> Get up and running with the AI Manager in 5 minutes. Essential patterns and examples for immediate productivity.

## 5-Minute Quick Start

### Installation
```bash
# Install AI pillar
pip install xlibrary[ai]

# Or install specific providers
pip install xlibrary[claude]    # Just Claude
pip install xlibrary[openai]    # Just OpenAI
pip install xlibrary[deepseek]  # Just DeepSeek
```

### Basic Usage

The AI Manager is designed for simplicity with powerful defaults. Here's how to get started:

```python
from xlibrary.ai import AIManager

# Minimal setup - API key is the only requirement
# This creates an AI manager using Claude (default provider) with the latest model
ai = AIManager(api_key="your-api-key")

# Make a request using the fluent API
# The .send() method executes the request and returns an AIResponse object
response = ai.request("Hello, world!").send()
print(response.content)  # "Hello! How can I help you today?"
```

**What's happening here:**
1. **AIManager** is initialized with just your API key - it defaults to Claude with the latest model
2. **request()** creates a chainable request object that you can modify before sending
3. **send()** executes the request and returns the AI's response
4. **response.content** contains the AI's text response

### API Key Setup

The AI Manager requires explicit API key configuration for security and clarity:

```python
# Minimal configuration - uses defaults (Claude + latest model)
ai = AIManager(api_key="your-api-key-here")

# Explicit configuration - specify provider and model
ai = AIManager(
    api_key="your-api-key-here",
    provider="claude",           # Optional: defaults to "claude"
    model="current"             # Optional: defaults to "latest"
)
```

**Why explicit API keys?**
- **Security**: No hidden environment variable dependencies
- **Clarity**: Always obvious where credentials come from
- **Flexibility**: Easy to use different keys for different instances
- **Testing**: Simple to mock or use test keys

**Note**: Environment variable fallbacks are intentionally not supported to maintain explicit configuration.

---

## Essential Patterns

### 1. Provider Switching

The AI Manager provides a unified interface that works identically across all AI providers. This allows you to switch between Claude, OpenAI, and DeepSeek without changing your code logic:

```python
# Use Claude (Anthropic)
# Claude excels at reasoning, analysis, and code generation
claude_ai = AIManager(api_key="claude-key", provider="claude", model="current")
claude_response = claude_ai.request("Explain Python decorators").send()

# Switch to OpenAI - identical interface, different AI model
# OpenAI/GPT-4 is strong at creative tasks and has structured output features
openai_ai = AIManager(api_key="openai-key", provider="openai", model="current")
openai_response = openai_ai.request("Explain Python decorators").send()

# Switch to DeepSeek - same code pattern
# DeepSeek offers competitive performance with reasoning capabilities
deepseek_ai = AIManager(api_key="deepseek-key", provider="deepseek", model="current")
deepseek_response = deepseek_ai.request("Explain Python decorators").send()
```

**Benefits of provider abstraction:**
- Test with different AI models to find the best fit for your use case
- Avoid vendor lock-in - easily switch providers based on cost, performance, or availability
- Use different providers for different tasks (e.g., Claude for analysis, OpenAI for creativity)

### 2. Universal Model Aliases

Instead of memorizing provider-specific model names (like "claude-3-5-sonnet-20241022" or "gpt-4o"), use universal aliases that work across all providers:

```python
# These aliases work with ALL providers and always map to the best available model:
ai_latest = AIManager(api_key="key", provider="claude", model="latest")      # Flagship model (best overall)
ai_current = AIManager(api_key="key", provider="openai", model="current")    # High-performance general use
ai_fast = AIManager(api_key="key", provider="deepseek", model="fast")        # Speed-optimized (lower cost)
ai_reasoning = AIManager(api_key="key", provider="claude", model="reasoning") # Thinking/reasoning capable

# Or use the default (latest model with Claude):
ai_default = AIManager(api_key="key")  # Equivalent to Claude + latest
```

**Model alias benefits:**
- **Consistency**: Same names work across all providers
- **Future-proof**: Aliases automatically update to newest models
- **Simplicity**: No need to track provider-specific model names
- **Optimization**: Choose the right performance/cost tradeoff for your use case

### 3. Stateless vs Stateful Requests

The AI Manager supports both independent requests and conversational interactions:

**Stateless** - Each request is completely independent:
```python
ai = AIManager(api_key="your-key")

# Each request starts fresh with no memory of previous interactions
response1 = ai.request("What is Python?").send()
response2 = ai.request("Give me an example").send()  # AI doesn't know what "example" refers to
```

**Stateful** - Conversation with memory:
```python
ai = AIManager(api_key="your-key")

# Create a conversation that maintains context between requests
conversation = ai.start_conversation()

# First request establishes context
response1 = conversation.request("What is Python?").send()

# Second request can reference previous context - the AI knows "example" means Python example
response2 = conversation.request("Give me an example").send()
```

**When to use each approach:**
- **Stateless**: Single questions, batch processing, parallel requests, API endpoints
- **Stateful**: Multi-turn conversations, tutorials, debugging sessions, interactive applications

### 4. Streaming Responses

For long responses, streaming provides real-time feedback as the AI generates content:

```python
ai = AIManager(api_key="your-key")

# Stream the response as it's generated - great for user interfaces
# Each chunk contains a piece of the response as it's produced by the AI
for chunk in ai.request_stream("Write a short story about AI"):
    print(chunk.content, end="", flush=True)  # Print each chunk immediately
print()  # Add final newline when streaming completes
```

**Benefits of streaming:**
- **Immediate feedback**: Users see response start appearing right away
- **Better UX**: Reduces perceived latency for long responses
- **Interruptible**: Can stop processing early if needed
- **Memory efficient**: Process large responses without loading everything into memory

### 5. File Attachments and Method Chaining

The AI Manager supports file attachments and provides a fluent API for complex requests:

```python
ai = AIManager(api_key="your-key", model="current")  # Use current model (has vision capabilities)

# Simple file attachment using method chaining
# The attach_file() method adds the file to the conversation context
response = ai.request("What's in this image?") \
    .attach_file("path/to/image.jpg") \
    .send()

# Complex chained request with multiple parameters
response = ai.request("Analyze these files and provide insights") \
    .attach_file("data.csv", "Sales data from Q4") \
    .attach_file("report.pdf", "Previous analysis") \
    .with_temperature(0.7) \
    .with_max_tokens(2000) \
    .send()

# Or use conversations for file-heavy workflows
conversation = ai.start_conversation()
conversation.add_file("document.pdf", "Main document to analyze")
response = conversation.request("Summarize the key points").send()
```

**File attachment features:**
- **Vision support**: Images (JPG, PNG) with capable models like Claude and GPT-4o
- **Document support**: PDFs, text files, code files
- **Method chaining**: Fluent API for complex requests
- **Context preservation**: Files remain available throughout conversations

---

## Common Use Cases

### 1. Simple Q&A Bot

```python
from xlibrary.ai import AIManager

def create_qa_bot():
    ai = AIManager(provider="claude", model="fast")  # Use fast model for cost efficiency

    while True:
        question = input("Ask me anything (or 'quit' to exit): ")
        if question.lower() == 'quit':
            break

        response = ai.request(question)
        print(f"AI: {response.content}\n")

if __name__ == "__main__":
    create_qa_bot()
```

### 2. Document Analysis

```python
def analyze_document(file_path: str):
    ai = AIManager(provider="claude", model="current")

    response = ai.request(
        "Analyze this document and provide a summary with key points",
        attachments=[file_path]
    )

    return response.content

# Usage
summary = analyze_document("report.pdf")
print(summary)
```

### 3. Multi-Provider Comparison

```python
def compare_providers(question: str):
    providers = ["claude", "openai", "deepseek"]
    results = {}

    for provider in providers:
        try:
            ai = AIManager(provider=provider, model="current")
            response = ai.request(question)
            results[provider] = response.content
        except Exception as e:
            results[provider] = f"Error: {e}"

    return results

# Usage
question = "Explain machine learning in simple terms"
results = compare_providers(question)

for provider, answer in results.items():
    print(f"\n{provider.upper()}:")
    print(answer)
    print("-" * 50)
```

### 4. Interactive Assistant

```python
def interactive_assistant():
    ai = AIManager(provider="claude", model="current")
    conversation = ai.create_conversation()

    print("Interactive AI Assistant (type 'exit' to quit)")
    print("-" * 50)

    while True:
        user_input = input("You: ")

        if user_input.lower() in ['exit', 'quit', 'bye']:
            print("AI: Goodbye!")
            break

        conversation.add_message(user_input)
        response = conversation.send()
        print(f"AI: {response.content}\n")

if __name__ == "__main__":
    interactive_assistant()
```

### 5. Reasoning Mode Example

```python
def solve_complex_problem(problem: str):
    ai = AIManager(provider="claude", model="reasoning")  # Or openai with o1-preview

    response = ai.request(
        f"Think step by step to solve this problem: {problem}",
        reasoning_mode=True  # Enable thinking process
    )

    print("Solution:", response.content)

    # Some models provide the reasoning process
    if hasattr(response, 'reasoning') and response.reasoning:
        print("\nThinking process:")
        print(response.reasoning)

# Usage
solve_complex_problem("How can I optimize a Python function that processes 1M records?")
```

---

## Error Handling Patterns

### Basic Error Handling

```python
from xlibrary.ai import AIManager, MissingCredentialsError, ProviderError

try:
    ai = AIManager(provider="claude")
    response = ai.request("Hello")
    print(response.content)

except MissingCredentialsError as e:
    print(f"Setup needed: {e.solution}")

except ProviderError as e:
    print(f"Provider issue: {e}")

except Exception as e:
    print(f"Unexpected error: {e}")
```

### Robust Error Handling with Fallbacks

```python
def robust_ai_request(message: str) -> str:
    providers = ["claude", "openai", "deepseek"]

    for provider in providers:
        try:
            ai = AIManager(provider=provider, model="current")
            response = ai.request(message)
            return f"[{provider}] {response.content}"

        except MissingCredentialsError:
            print(f"Skipping {provider} - no credentials")
            continue

        except ProviderError as e:
            print(f"{provider} failed: {e}")
            continue

    return "All providers failed"

# Usage
result = robust_ai_request("What is the weather like?")
print(result)
```

---

## Configuration Examples

### Basic Configuration

```python
from xlibrary.ai import AIManager, AIConfig

# Method 1: Direct parameters
ai = AIManager(
    provider="claude",
    model="current",
    api_key="your-key",
    max_tokens=2000,
    temperature=0.7
)

# Method 2: Configuration object
config = AIConfig(
    provider="openai",
    model="gpt-4o",
    max_tokens=1500,
    temperature=0.8,
    timeout=45.0
)

ai = AIManager(config=config)
```

### Environment-Based Configuration

```python
import os
from xlibrary.ai import AIManager

# Set up environment
os.environ['ANTHROPIC_API_KEY'] = 'your-claude-key'
os.environ['OPENAI_API_KEY'] = 'your-openai-key'

# Auto-detects API keys from environment
claude_ai = AIManager(provider="claude")
openai_ai = AIManager(provider="openai")
```

---

## Testing and Validation

### Using Mock Provider for Testing

```python
from xlibrary.ai import AIManager

def test_my_ai_function():
    # Use mock provider for testing - no API key needed
    ai = AIManager(provider="mock", model="latest")

    response = ai.request("Test message")
    print(response.content)  # Mock response

    # Mock provider returns predictable responses
    assert "mock response" in response.content.lower()

# Test your AI-powered functions without real API calls
test_my_ai_function()
```

### Validate Setup

```python
from xlibrary.ai import AIManager

def validate_setup():
    providers = ["claude", "openai", "deepseek"]

    for provider in providers:
        try:
            ai = AIManager(provider=provider)
            models = ai.get_models()
            print(f"✅ {provider}: {len(models)} models available")

        except Exception as e:
            print(f"❌ {provider}: {e}")

validate_setup()
```

---

## Best Practices

### 1. **Resource Management**
```python
# Good: Reuse AIManager instances
ai = AIManager(provider="claude")
for question in questions:
    response = ai.request(question)  # Reuses connection

# Avoid: Creating new instances for each request
for question in questions:
    ai = AIManager(provider="claude")  # Inefficient
    response = ai.request(question)
```

### 2. **Error Handling**
```python
# Always handle credential errors gracefully
try:
    ai = AIManager(provider="claude")
except MissingCredentialsError as e:
    print(f"Please set up credentials: {e.solution}")
    exit(1)
```

### 3. **Model Selection**
```python
# Use appropriate models for tasks
ai_fast = AIManager(provider="claude", model="fast")      # Simple tasks
ai_current = AIManager(provider="claude", model="current") # Complex tasks
ai_reasoning = AIManager(provider="claude", model="reasoning") # Problem solving
```

### 4. **Production Configuration**
```python
# Enable production features for monitoring
ai = AIManager(
    provider="claude",
    model="current",
    enable_metrics=True,      # Track usage
    enable_health_checks=True, # Monitor provider health
    enable_logging=True       # Audit trail
)
```

---

## Next Steps

Ready for more advanced features?

- **[Detailed User Guide](01.04%20Chapter%201%20-%20AI%20Manager%20-%20User%20Guide%20-%20Detailed.md)** - Comprehensive feature coverage
- **[Chapter 2: Config Manager](02.01%20Chapter%202%20-%20Config%20Manager%20-%20Design%20-%20Overview.md)** - Configuration management
- **[Design Documentation](01.01%20Chapter%201%20-%20AI%20Manager%20-%20Design%20-%20Overview.md)** - Architectural details

**You're now ready to build AI-powered applications with xlibrary!** 🎉

---

## Troubleshooting

### Common Issues

**"Missing API key" error:**
```bash
# Set environment variable
export ANTHROPIC_API_KEY="your-key-here"

# Or pass directly
ai = AIManager(provider="claude", api_key="your-key")
```

**Import errors:**
```bash
# Install required dependencies
pip install xlibrary[ai]      # All AI providers
pip install xlibrary[claude]  # Just Claude
```

**Provider not responding:**
```python
# Check provider health
ai = AIManager(provider="claude", enable_health_checks=True)
health = ai.check_health()
print(health.status)
```