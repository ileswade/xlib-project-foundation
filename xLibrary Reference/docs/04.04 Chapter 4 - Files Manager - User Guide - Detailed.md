# Chapter 4: Files Manager - User Guide Detailed

> **üìÅ COMPREHENSIVE FILE OPERATIONS MASTERY**
> Complete guide to xlibrary's Files Manager: advanced file type detection, sophisticated deduplication, multi-format compression, intelligent organization, and enterprise file operations.

## Advanced File Operations

### Comprehensive File Analysis

**What this example demonstrates:** How to perform deep security-focused file analysis that goes far beyond basic file properties. This pattern is essential for security audits, forensic analysis, and ensuring file integrity in enterprise applications.

**Key concepts to notice:**
- Multi-layer file type detection (extension vs actual content)
- Security analysis with permission checking
- Hash calculation for integrity verification
- Type mismatch detection for security threats
- Comprehensive metadata extraction

**When to use this:** Security auditing, malware detection, file integrity verification, or when you need complete file analysis for compliance or forensic purposes.

The Files Manager provides deep file analysis beyond basic properties:

```python
from xlibrary.files import FileManager
from pathlib import Path

fm = FileManager()

# Complete file analysis
def analyze_file_comprehensive(file_path):
    """Get comprehensive file analysis."""
    file_info = fm.detect_file_type(file_path)

    print(f"üìÑ File Analysis: {file_info.name}")
    print(f"=" * 50)

    # Basic properties
    print(f"Path: {file_info.path}")
    print(f"Size: {file_info.size:,} bytes ({file_info.size / 1024**2:.2f} MB)")
    print(f"Extension: {file_info.extension}")
    print(f"Created: {file_info.created_time}")
    print(f"Modified: {file_info.modified_time}")

    # Security analysis
    print(f"Hidden: {file_info.is_hidden}")
    print(f"Readable: {file_info.is_readable}")
    print(f"Writable: {file_info.is_writable}")
    print(f"Executable: {file_info.is_executable}")
    print(f"Permissions: {file_info.permissions}")

    # Type detection
    print(f"Declared type: {file_info.declared_type}")
    print(f"Actual type: {file_info.actual_type}")
    print(f"MIME type: {file_info.mime_type}")
    print(f"File category: {file_info.file_type.value}")

    # Security flags
    if file_info.is_type_mismatch:
        print("‚ö†Ô∏è  WARNING: File extension doesn't match content!")
        print("   This could indicate:")
        print("   - Renamed file")
        print("   - Corrupted file")
        print("   - Potential security threat")

    # Hashes for verification
    if file_info.hash_md5:
        print(f"MD5: {file_info.hash_md5}")
    if file_info.hash_sha256:
        print(f"SHA256: {file_info.hash_sha256}")

# Usage
analyze_file_comprehensive("/path/to/suspicious/file.exe")
```

### Advanced File Type Detection

**What this example demonstrates:** How to implement batch file analysis across entire directory structures for security and organization purposes. This approach is valuable for system administrators, security professionals, and anyone managing large file repositories.

**Key concepts to notice:**
- Recursive directory traversal using `Path.rglob('*')`
- Statistical analysis of file type distribution
- Automated detection of type mismatches (security concern)
- Classification of suspicious files based on patterns
- Comprehensive reporting for decision making

**When to use this:** System audits, security scans, file organization planning, malware detection, or compliance reporting.

```python
from xlibrary.files import FileManager, FileType

class AdvancedTypeDetector:
    def __init__(self):
        self.fm = FileManager()

    def batch_analyze_types(self, directory):
        """Analyze file types in entire directory."""
        type_stats = {}
        mismatch_files = []
        suspicious_files = []

        for file_path in Path(directory).rglob('*'):
            if file_path.is_file():
                file_info = self.fm.detect_file_type(file_path)

                # Count file types
                file_type = file_info.file_type
                if file_type not in type_stats:
                    type_stats[file_type] = {'count': 0, 'size': 0}
                type_stats[file_type]['count'] += 1
                type_stats[file_type]['size'] += file_info.size

                # Track mismatches
                if file_info.is_type_mismatch:
                    mismatch_files.append({
                        'path': file_path,
                        'declared': file_info.declared_type,
                        'actual': file_info.actual_type
                    })

                # Flag suspicious files
                if self._is_suspicious(file_info):
                    suspicious_files.append(file_path)

        return {
            'type_statistics': type_stats,
            'type_mismatches': mismatch_files,
            'suspicious_files': suspicious_files
        }

    def _is_suspicious(self, file_info):
        """Check if file has suspicious characteristics."""
        suspicious_indicators = [
            # Executable masquerading as document
            file_info.file_type == FileType.EXECUTABLE and
            file_info.extension in ['.pdf', '.doc', '.txt'],

            # Hidden executable
            file_info.is_hidden and file_info.file_type == FileType.EXECUTABLE,

            # Large file with suspicious extension
            file_info.size > 100 * 1024 * 1024 and  # >100MB
            file_info.extension in ['.txt', '.log'],

            # Binary file with text extension
            file_info.file_type == FileType.BINARY and
            file_info.extension in ['.txt', '.csv', '.xml']
        ]

        return any(suspicious_indicators)

# Usage
detector = AdvancedTypeDetector()
analysis = detector.batch_analyze_types("/suspicious/directory")

print("üîç Type Analysis Results:")
for file_type, stats in analysis['type_statistics'].items():
    print(f"{file_type.value}: {stats['count']} files, {stats['size'] / 1024**2:.1f} MB")

if analysis['type_mismatches']:
    print("‚ö†Ô∏è  Type Mismatches Found:")
    for mismatch in analysis['type_mismatches']:
        print(f"  {mismatch['path']}: {mismatch['declared']} ‚Üí {mismatch['actual']}")

if analysis['suspicious_files']:
    print("üö® Suspicious Files:")
    for suspicious in analysis['suspicious_files']:
        print(f"  {suspicious}")
```

### Sophisticated Deduplication

**What this example demonstrates:** How to implement intelligent duplicate file detection and removal with multiple strategies. This is crucial for storage optimization, system cleanup, and managing large file repositories efficiently.

**Key concepts to notice:**
- Multiple detection strategies (size-based, hash-based)
- Statistical analysis of duplicate findings
- Strategy pattern for different removal approaches
- Safe removal with error handling and rollback capability
- Comprehensive reporting of space savings

**How the deduplication works:**
1. **Detection Phase**: Scans directory recursively, comparing files by size first (fast), then by hash (accurate)
2. **Analysis Phase**: Groups duplicates and calculates potential space savings
3. **Strategy Selection**: Chooses which files to keep based on business rules (newest, oldest, location, etc.)
4. **Safe Removal**: Removes duplicates with error handling and progress tracking

**When to use this:** Storage cleanup, backup optimization, managing photo/video libraries, or any scenario where disk space is at a premium.

```python
from xlibrary.files import FileManager
from datetime import datetime, timedelta

class SmartDeduplicator:
    def __init__(self):
        self.fm = FileManager()

    def find_duplicates_advanced(self, directory, strategies=['size', 'hash']):
        """Advanced duplicate detection with multiple strategies."""

        print(f"üîç Scanning {directory} for duplicates...")
        duplicates = self.fm.find_duplicates(directory, methods=strategies)

        # Analyze duplicate groups
        total_groups = len(duplicates)
        total_files = sum(len(group.files) for group in duplicates)
        total_wasted = sum(group.wasted_space for group in duplicates)

        print(f"Found {total_groups} duplicate groups with {total_files} total files")
        print(f"Potential space savings: {total_wasted / 1024**2:.1f} MB")

        return duplicates

    def smart_duplicate_removal(self, duplicate_groups, strategy='interactive'):
        """Smart duplicate removal with various strategies."""

        strategies = {
            'keep_newest': self._keep_newest,
            'keep_oldest': self._keep_oldest,
            'keep_largest': self._keep_largest,
            'keep_in_primary_location': self._keep_in_primary_location,
            'interactive': self._interactive_choice
        }

        if strategy not in strategies:
            raise ValueError(f"Unknown strategy: {strategy}")

        removal_func = strategies[strategy]
        removed_files = []
        space_recovered = 0

        for group in duplicate_groups:
            files_to_remove = removal_func(group)

            for file_path in files_to_remove:
                try:
                    size = file_path.stat().st_size
                    file_path.unlink()
                    removed_files.append(file_path)
                    space_recovered += size
                    print(f"Removed: {file_path}")
                except Exception as e:
                    print(f"Failed to remove {file_path}: {e}")

        return {
            'removed_files': removed_files,
            'space_recovered': space_recovered,
            'files_removed_count': len(removed_files)
        }

    def _keep_newest(self, group):
        """Keep the newest file, remove others."""
        if len(group.files) <= 1:
            return []

        # Sort by modification time, newest first
        sorted_files = sorted(group.files,
                            key=lambda f: f.modified_time,
                            reverse=True)

        # Remove all except the first (newest)
        return [f.path for f in sorted_files[1:]]

    def _keep_oldest(self, group):
        """Keep the oldest file, remove others."""
        if len(group.files) <= 1:
            return []

        # Sort by modification time, oldest first
        sorted_files = sorted(group.files,
                            key=lambda f: f.modified_time)

        # Remove all except the first (oldest)
        return [f.path for f in sorted_files[1:]]

    def _keep_in_primary_location(self, group):
        """Keep file in primary location (shortest path), remove others."""
        if len(group.files) <= 1:
            return []

        # Sort by path length (shorter = more primary)
        sorted_files = sorted(group.files,
                            key=lambda f: len(str(f.path)))

        # Remove all except the first (shortest path)
        return [f.path for f in sorted_files[1:]]

    def _interactive_choice(self, group):
        """Let user choose which file to keep."""
        if len(group.files) <= 1:
            return []

        print(f"\nüîÑ Duplicate group ({len(group.files)} files):")
        for i, file_info in enumerate(group.files):
            size_mb = file_info.size / 1024**2
            print(f"{i+1:2d}. {file_info.path}")
            print(f"     Size: {size_mb:.2f} MB, Modified: {file_info.modified_time}")

        while True:
            choice = input(f"Keep which file? (1-{len(group.files)}, or 's' to skip): ")

            if choice.lower() == 's':
                return []  # Skip this group

            try:
                keep_index = int(choice) - 1
                if 0 <= keep_index < len(group.files):
                    # Return all files except the chosen one
                    return [f.path for i, f in enumerate(group.files) if i != keep_index]
                else:
                    print(f"Please enter 1-{len(group.files)} or 's'")
            except ValueError:
                print("Please enter a number or 's'")

# Usage
deduplicator = SmartDeduplicator()

# Find duplicates
duplicates = deduplicator.find_duplicates_advanced("/photos")

# Remove duplicates with different strategies
result = deduplicator.smart_duplicate_removal(duplicates, 'keep_newest')
print(f"Removed {result['files_removed_count']} duplicate files")
print(f"Recovered {result['space_recovered'] / 1024**2:.1f} MB")
```

### Advanced Folder Organization

**What this example demonstrates:** How to implement intelligent file organization using metadata extraction and content analysis. This approach automatically sorts files into meaningful folder structures based on their content, metadata, and context rather than just file types.

**Key concepts to notice:**
- Rule-based organization using custom functions
- Metadata extraction from images (EXIF data), videos (resolution), and audio (ID3 tags)
- Fallback strategies when metadata is unavailable
- Dynamic folder structure creation based on content
- Content analysis for intelligent document categorization

**How the organization works:**
1. **File Analysis**: Examines each file to extract relevant metadata
2. **Rule Application**: Applies custom organization rules based on file type and content
3. **Path Generation**: Creates meaningful folder paths (e.g., "Photos/2024/03", "Videos/4K/MP4")
4. **Safe Movement**: Moves files while preserving structure and handling conflicts

**When to use this:** Managing large media collections, organizing downloads, maintaining document archives, or any scenario requiring intelligent file sorting beyond basic type-based organization.

```python
from xlibrary.files import FileManager, FileType
from pathlib import Path
import re

class IntelligentOrganizer:
    def __init__(self):
        self.fm = FileManager()

    def organize_media_by_metadata(self, source_dir):
        """Organize media files by extracted metadata."""

        def media_organization_rule(file_info):
            if file_info.file_type == FileType.IMAGE:
                # Try to extract date from EXIF or filename
                date_str = self._extract_date_from_image(file_info.path)
                if date_str:
                    return f"Photos/{date_str[:4]}/{date_str[4:6]}"  # Year/Month
                else:
                    return f"Photos/{file_info.modified_time.year}"

            elif file_info.file_type == FileType.VIDEO:
                # Organize videos by resolution and format
                resolution = self._detect_video_resolution(file_info.path)
                return f"Videos/{resolution}/{file_info.extension.upper()[1:]}"  # Remove dot

            elif file_info.file_type == FileType.AUDIO:
                # Try to organize by genre/artist from metadata
                metadata = self._extract_audio_metadata(file_info.path)
                if metadata.get('artist'):
                    return f"Music/{metadata['artist']}"
                else:
                    return "Music/Unknown Artist"

            return "Other"

        result = self.fm.organize_by_rule(source_dir, media_organization_rule)
        return result

    def organize_documents_intelligently(self, source_dir):
        """Organize documents by content analysis."""

        def document_organization_rule(file_info):
            if file_info.file_type != FileType.DOCUMENT:
                return None  # Skip non-documents

            # Analyze document content/name for patterns
            content_type = self._analyze_document_content(file_info.path)

            if content_type:
                return f"Documents/{content_type}"
            else:
                # Fallback to extension-based organization
                return f"Documents/{file_info.extension.upper()[1:]}"

        result = self.fm.organize_by_rule(source_dir, document_organization_rule)
        return result

    def organize_by_project_structure(self, source_dir, project_patterns):
        """Organize files based on project structure patterns."""

        def project_rule(file_info):
            file_path_str = str(file_info.path)

            for pattern, destination in project_patterns.items():
                if re.search(pattern, file_path_str, re.IGNORECASE):
                    return destination

            # Default organization by type
            return f"Unsorted/{file_info.file_type.value.title()}"

        result = self.fm.organize_by_rule(source_dir, project_rule)
        return result

    def _extract_date_from_image(self, image_path):
        """Extract date from image metadata or filename."""
        # Try filename patterns first (YYYYMMDD, YYYY-MM-DD, etc.)
        filename = image_path.name

        # Pattern: YYYYMMDD
        match = re.search(r'(20\d{2})(\d{2})(\d{2})', filename)
        if match:
            return f"{match.group(1)}{match.group(2)}{match.group(3)}"

        # Pattern: YYYY-MM-DD
        match = re.search(r'(20\d{2})-(\d{2})-(\d{2})', filename)
        if match:
            return f"{match.group(1)}{match.group(2)}{match.group(3)}"

        # TODO: Add EXIF date extraction
        return None

    def _detect_video_resolution(self, video_path):
        """Detect video resolution (simplified)."""
        # This would require video processing library like ffmpeg-python
        # For now, use file size as proxy
        size = video_path.stat().st_size

        if size > 1024**3:  # > 1GB
            return "4K"
        elif size > 500 * 1024**2:  # > 500MB
            return "1080p"
        elif size > 100 * 1024**2:  # > 100MB
            return "720p"
        else:
            return "480p"

    def _extract_audio_metadata(self, audio_path):
        """Extract audio metadata (simplified)."""
        # This would require audio metadata library like mutagen
        # For now, try to extract from filename
        filename = audio_path.name

        # Pattern: Artist - Song.ext
        match = re.match(r'^(.+?)\s*-\s*(.+?)\.', filename)
        if match:
            return {'artist': match.group(1).strip()}

        return {}

    def _analyze_document_content(self, doc_path):
        """Analyze document content to determine type."""
        filename_lower = doc_path.name.lower()

        # Financial documents
        if any(word in filename_lower for word in ['invoice', 'receipt', 'tax', 'bank', 'statement']):
            return "Financial"

        # Legal documents
        if any(word in filename_lower for word in ['contract', 'agreement', 'legal', 'terms']):
            return "Legal"

        # Personal documents
        if any(word in filename_lower for word in ['resume', 'cv', 'passport', 'id', 'certificate']):
            return "Personal"

        # Work documents
        if any(word in filename_lower for word in ['meeting', 'project', 'proposal', 'report']):
            return "Work"

        return None

# Usage examples
organizer = IntelligentOrganizer()

# Organize photos by date
result = organizer.organize_media_by_metadata("/Users/john/Pictures")

# Organize documents by content
result = organizer.organize_documents_intelligently("/Users/john/Documents")

# Organize by custom project patterns
project_patterns = {
    r'project[_-]alpha': 'Projects/Alpha',
    r'client[_-]bravo': 'Clients/Bravo',
    r'\.psd$': 'Design/Photoshop',
    r'\.sketch$': 'Design/Sketch',
    r'screenshot': 'Screenshots'
}

result = organizer.organize_by_project_structure("/messy/work/folder", project_patterns)
```

### Comprehensive Compression Operations

```python
from xlibrary.files import FileManager, CompressionFormat
from pathlib import Path
import threading
import time

class AdvancedCompressionManager:
    def __init__(self):
        self.fm = FileManager()
        self.active_compressions = {}

    def compress_with_options(self, source, destination, options):
        """Compress with advanced options."""

        # Determine optimal compression format
        if not options.get('format'):
            options['format'] = self._choose_optimal_format(source)

        # Set compression level based on speed/size preference
        if 'compression_level' not in options:
            if options.get('speed_priority'):
                options['compression_level'] = 1  # Fast
            elif options.get('size_priority'):
                options['compression_level'] = 9  # Best compression
            else:
                options['compression_level'] = 6  # Balanced

        # Create progress tracking
        progress_callback = self._create_progress_tracker(source, destination)

        result = self.fm.compress_folder(
            source,
            destination,
            format=options['format'],
            compression_level=options['compression_level'],
            progress_callback=progress_callback
        )

        return result

    def batch_compress_folders(self, folder_list, base_output_dir, options=None):
        """Compress multiple folders in parallel."""
        options = options or {}
        results = {}
        threads = []

        def compress_single(source_folder, output_path):
            """Compress single folder in thread."""
            try:
                result = self.compress_with_options(source_folder, output_path, options)
                results[str(source_folder)] = result
            except Exception as e:
                results[str(source_folder)] = {'success': False, 'error': str(e)}

        # Start compression threads
        for source_folder in folder_list:
            source_path = Path(source_folder)
            output_filename = f"{source_path.name}.{options.get('format', 'zip')}"
            output_path = Path(base_output_dir) / output_filename

            thread = threading.Thread(
                target=compress_single,
                args=(source_path, output_path)
            )
            threads.append(thread)
            thread.start()

        # Wait for all compressions to complete
        for thread in threads:
            thread.join()

        return results

    def intelligent_compression_strategy(self, directory):
        """Choose compression strategy based on content analysis."""

        # Analyze directory contents
        file_types = {}
        total_size = 0
        file_count = 0

        for file_path in Path(directory).rglob('*'):
            if file_path.is_file():
                file_info = self.fm.detect_file_type(file_path)
                file_type = file_info.file_type

                if file_type not in file_types:
                    file_types[file_type] = {'count': 0, 'size': 0}

                file_types[file_type]['count'] += 1
                file_types[file_type]['size'] += file_info.size

                total_size += file_info.size
                file_count += 1

        # Choose strategy based on content
        strategy = self._analyze_compression_strategy(file_types, total_size, file_count)

        print(f"üìä Compression Analysis:")
        print(f"Total files: {file_count:,}")
        print(f"Total size: {total_size / 1024**3:.2f} GB")
        print(f"Recommended strategy: {strategy['description']}")

        return strategy

    def _choose_optimal_format(self, source_path):
        """Choose optimal compression format based on content."""

        # Quick content analysis
        has_many_files = sum(1 for _ in Path(source_path).rglob('*')) > 1000
        total_size = sum(f.stat().st_size for f in Path(source_path).rglob('*') if f.is_file())

        if total_size > 2 * 1024**3:  # > 2GB
            return 'tar.xz'  # Best compression for large archives
        elif has_many_files:
            return 'tar.gz'  # Good balance for many files
        else:
            return 'zip'     # Good compatibility

    def _analyze_compression_strategy(self, file_types, total_size, file_count):
        """Analyze content and recommend compression strategy."""

        # Calculate percentages
        type_percentages = {}
        for file_type, stats in file_types.items():
            type_percentages[file_type] = (stats['size'] / total_size) * 100

        # Strategy recommendations
        if type_percentages.get('IMAGE', 0) > 60:
            return {
                'format': 'zip',
                'compression_level': 3,  # Images don't compress much
                'description': 'Fast compression for image-heavy content'
            }

        elif type_percentages.get('TEXT', 0) > 50:
            return {
                'format': 'tar.xz',
                'compression_level': 9,  # Text compresses very well
                'description': 'Maximum compression for text content'
            }

        elif type_percentages.get('VIDEO', 0) > 40:
            return {
                'format': 'tar',
                'compression_level': 1,  # Videos already compressed
                'description': 'Fast archiving for video content'
            }

        elif total_size > 5 * 1024**3:  # > 5GB
            return {
                'format': 'tar.gz',
                'compression_level': 6,
                'description': 'Balanced compression for large archive'
            }

        else:
            return {
                'format': 'zip',
                'compression_level': 6,
                'description': 'Standard compression with good compatibility'
            }

    def _create_progress_tracker(self, source, destination):
        """Create progress tracking callback."""

        start_time = time.time()

        def progress_callback(current, total, message):
            elapsed = time.time() - start_time
            if current > 0:
                eta = (elapsed / current) * (total - current)
                eta_str = f"ETA: {eta:.0f}s"
            else:
                eta_str = "ETA: calculating..."

            percent = (current / total) * 100 if total > 0 else 0

            print(f"\rüì¶ [{percent:5.1f}%] {message} - {eta_str}", end="", flush=True)

            if current == total:
                print()  # New line when complete

        return progress_callback

# Usage examples
compressor = AdvancedCompressionManager()

# Intelligent compression with analysis
strategy = compressor.intelligent_compression_strategy("/large/project")
result = compressor.compress_with_options(
    "/large/project",
    "/backups/project_backup.tar.xz",
    strategy
)

# Batch compress multiple folders
folders_to_compress = [
    "/projects/alpha",
    "/projects/beta",
    "/projects/gamma"
]

batch_results = compressor.batch_compress_folders(
    folders_to_compress,
    "/backup/archives",
    {'format': 'tar.gz', 'size_priority': True}
)

for folder, result in batch_results.items():
    if result.get('success'):
        print(f"‚úÖ {folder}: compressed successfully")
    else:
        print(f"‚ùå {folder}: {result.get('error')}")
```

### Automated File System Maintenance

```python
from xlibrary.files import FileManager
from pathlib import Path
from datetime import datetime, timedelta
import json
import logging

class FileSystemMaintainer:
    def __init__(self, config_file="file_maintenance.json"):
        self.fm = FileManager()
        self.config_file = config_file
        self.config = self._load_config()
        self._setup_logging()

    def _load_config(self):
        """Load maintenance configuration."""
        default_config = {
            "directories_to_monitor": [],
            "cleanup_rules": {
                "remove_empty_folders": True,
                "remove_old_files_days": 365,
                "remove_duplicates": True,
                "organize_by_type": False
            },
            "exclusions": {
                "protected_extensions": [".system", ".lock"],
                "protected_folders": ["System", "Library", ".git"]
            },
            "scheduling": {
                "auto_maintenance": False,
                "maintenance_hour": 2,  # 2 AM
                "maintenance_days": ["sunday"]
            }
        }

        try:
            with open(self.config_file, 'r') as f:
                config = json.load(f)
            # Merge with defaults
            return {**default_config, **config}
        except FileNotFoundError:
            return default_config

    def _setup_logging(self):
        """Setup maintenance logging."""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('file_maintenance.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

    def run_full_maintenance(self):
        """Run complete file system maintenance."""

        self.logger.info("Starting full file system maintenance")

        maintenance_results = {
            'start_time': datetime.now(),
            'directories_processed': [],
            'total_space_recovered': 0,
            'operations_performed': [],
            'errors': []
        }

        for directory in self.config['directories_to_monitor']:
            try:
                dir_results = self._maintain_directory(directory)
                maintenance_results['directories_processed'].append(dir_results)
                maintenance_results['total_space_recovered'] += dir_results.get('space_recovered', 0)

            except Exception as e:
                error_msg = f"Failed to maintain {directory}: {str(e)}"
                self.logger.error(error_msg)
                maintenance_results['errors'].append(error_msg)

        maintenance_results['end_time'] = datetime.now()
        maintenance_results['duration'] = (
            maintenance_results['end_time'] - maintenance_results['start_time']
        ).total_seconds()

        self._save_maintenance_report(maintenance_results)
        return maintenance_results

    def _maintain_directory(self, directory):
        """Perform maintenance on single directory."""

        dir_path = Path(directory)
        if not dir_path.exists():
            raise ValueError(f"Directory does not exist: {directory}")

        self.logger.info(f"Maintaining directory: {directory}")

        results = {
            'directory': directory,
            'operations': [],
            'space_recovered': 0,
            'files_processed': 0
        }

        # Operation 1: Remove duplicates
        if self.config['cleanup_rules']['remove_duplicates']:
            duplicate_results = self._remove_duplicates_safe(dir_path)
            results['operations'].append(duplicate_results)
            results['space_recovered'] += duplicate_results.get('space_recovered', 0)

        # Operation 2: Remove old files
        old_file_days = self.config['cleanup_rules']['remove_old_files_days']
        if old_file_days > 0:
            old_file_results = self._remove_old_files(dir_path, old_file_days)
            results['operations'].append(old_file_results)
            results['space_recovered'] += old_file_results.get('space_recovered', 0)

        # Operation 3: Remove empty folders
        if self.config['cleanup_rules']['remove_empty_folders']:
            empty_folder_results = self._remove_empty_folders_safe(dir_path)
            results['operations'].append(empty_folder_results)

        # Operation 4: Organize by type (optional)
        if self.config['cleanup_rules']['organize_by_type']:
            organize_results = self._organize_directory_safe(dir_path)
            results['operations'].append(organize_results)

        return results

    def _remove_duplicates_safe(self, directory):
        """Safely remove duplicates with protection."""

        self.logger.info(f"Scanning for duplicates in {directory}")

        try:
            duplicates = self.fm.find_duplicates(directory, methods=['hash'])

            if not duplicates:
                return {'operation': 'remove_duplicates', 'files_removed': 0, 'space_recovered': 0}

            # Filter out protected files
            filtered_groups = []
            for group in duplicates:
                filtered_files = []
                for file_info in group.files:
                    if not self._is_protected_file(file_info.path):
                        filtered_files.append(file_info)

                if len(filtered_files) > 1:  # Still have duplicates after filtering
                    group.files = filtered_files
                    filtered_groups.append(group)

            # Remove duplicates (keep newest)
            removed_results = self.fm.remove_duplicates(filtered_groups, keep='newest')

            self.logger.info(f"Removed {removed_results['files_removed_count']} duplicate files, "
                           f"recovered {removed_results['space_recovered'] / 1024**2:.1f} MB")

            return {
                'operation': 'remove_duplicates',
                'files_removed': removed_results['files_removed_count'],
                'space_recovered': removed_results['space_recovered']
            }

        except Exception as e:
            self.logger.error(f"Duplicate removal failed: {str(e)}")
            return {'operation': 'remove_duplicates', 'error': str(e)}

    def _remove_old_files(self, directory, days_old):
        """Remove files older than specified days."""

        cutoff_date = datetime.now() - timedelta(days=days_old)
        old_files = []
        total_size = 0

        for file_path in directory.rglob('*'):
            if file_path.is_file() and not self._is_protected_file(file_path):
                try:
                    file_stat = file_path.stat()
                    file_modified = datetime.fromtimestamp(file_stat.st_mtime)

                    if file_modified < cutoff_date:
                        old_files.append(file_path)
                        total_size += file_stat.st_size

                except Exception:
                    continue  # Skip files we can't access

        # Remove old files
        removed_count = 0
        space_recovered = 0

        for file_path in old_files:
            try:
                file_size = file_path.stat().st_size
                file_path.unlink()
                removed_count += 1
                space_recovered += file_size
                self.logger.debug(f"Removed old file: {file_path}")

            except Exception as e:
                self.logger.warning(f"Failed to remove {file_path}: {str(e)}")

        self.logger.info(f"Removed {removed_count} old files (>{days_old} days), "
                        f"recovered {space_recovered / 1024**2:.1f} MB")

        return {
            'operation': 'remove_old_files',
            'files_removed': removed_count,
            'space_recovered': space_recovered,
            'days_threshold': days_old
        }

    def _is_protected_file(self, file_path):
        """Check if file is protected from maintenance operations."""

        path_str = str(file_path).lower()

        # Check protected extensions
        for ext in self.config['exclusions']['protected_extensions']:
            if path_str.endswith(ext.lower()):
                return True

        # Check protected folders
        for folder in self.config['exclusions']['protected_folders']:
            if folder.lower() in path_str:
                return True

        # Always protect system files
        system_indicators = ['.system', '.lock', '.tmp', 'thumbs.db', '.ds_store']
        if any(indicator in path_str for indicator in system_indicators):
            return True

        return False

    def _save_maintenance_report(self, results):
        """Save maintenance report to file."""

        report_file = f"maintenance_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

        # Convert datetime objects to strings for JSON serialization
        def json_serial(obj):
            if isinstance(obj, datetime):
                return obj.isoformat()
            raise TypeError(f"Type {type(obj)} not serializable")

        try:
            with open(report_file, 'w') as f:
                json.dump(results, f, indent=2, default=json_serial)

            self.logger.info(f"Maintenance report saved to {report_file}")

        except Exception as e:
            self.logger.error(f"Failed to save report: {str(e)}")

    def generate_maintenance_summary(self):
        """Generate summary of recent maintenance activities."""

        # Find recent maintenance reports
        report_files = sorted(Path('.').glob('maintenance_report_*.json'))

        if not report_files:
            print("No maintenance reports found")
            return

        latest_report = report_files[-1]

        try:
            with open(latest_report, 'r') as f:
                report = json.load(f)

            print("üßπ Latest Maintenance Summary")
            print("=" * 40)
            print(f"Date: {report['start_time']}")
            print(f"Duration: {report['duration']:.1f} seconds")
            print(f"Directories processed: {len(report['directories_processed'])}")
            print(f"Total space recovered: {report['total_space_recovered'] / 1024**2:.1f} MB")

            if report['errors']:
                print(f"Errors encountered: {len(report['errors'])}")
                for error in report['errors']:
                    print(f"  - {error}")

            print("\nDetailed Results:")
            for dir_result in report['directories_processed']:
                print(f"\nüìÅ {dir_result['directory']}")
                for op in dir_result['operations']:
                    if 'files_removed' in op:
                        print(f"  {op['operation']}: {op['files_removed']} files removed")
                    else:
                        print(f"  {op['operation']}: completed")

        except Exception as e:
            print(f"Failed to read report: {str(e)}")

# Usage
maintainer = FileSystemMaintainer()

# Configure directories to monitor
config = {
    "directories_to_monitor": [
        "/Users/username/Downloads",
        "/Users/username/Desktop",
        "/Users/username/Documents"
    ],
    "cleanup_rules": {
        "remove_duplicates": True,
        "remove_old_files_days": 180,  # 6 months
        "remove_empty_folders": True,
        "organize_by_type": False
    }
}

with open("file_maintenance.json", "w") as f:
    json.dump(config, f, indent=2)

# Run maintenance
results = maintainer.run_full_maintenance()

# View summary
maintainer.generate_maintenance_summary()
```

This comprehensive guide demonstrates the full power and flexibility of xlibrary's Files Manager. From basic file operations to sophisticated automation, it provides enterprise-grade file management capabilities with safety, intelligence, and efficiency at its core.

---

## Next Steps

- **[Chapter 5: Media Manager](05.01%20Chapter%205%20-%20Media%20Manager%20-%20Design%20-%20Overview.md)** - Media processing and manipulation
- **[Design Documentation](04.01%20Chapter%204%20-%20Files%20Manager%20-%20Design%20-%20Overview.md)** - Architecture deep dive
- **[xlibrary Core Concepts](00.00%20Chapter%200%20-%20Introduction.md)** - Understanding the xlibrary ecosystem

**Master file operations with xlibrary's comprehensive Files Manager!** üìÅ