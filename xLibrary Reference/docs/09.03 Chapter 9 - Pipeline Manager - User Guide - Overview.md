# Chapter 9: Pipeline Manager - User Guide - Overview

> **⚡ QUICK START GUIDE**
> Get up and running with xlibrary's Pipeline Manager in 5 minutes. Master data pipeline creation, task orchestration, and distributed processing with practical examples.

## Quick Setup

### 5-Minute Start

```bash
# Install with pipeline management
pip install xlibrary[pipeline]

# Verify installation
python -c "from xlibrary.pipeline import PipelineManager; print('Ready!')"
```

### Basic Configuration

```python
from xlibrary.pipeline import PipelineManager, Pipeline, Task

# Simple setup
pm = PipelineManager()

# Enterprise setup
pm = PipelineManager({
    "distributed_mode": True,
    "auto_scaling": True,
    "monitoring_enabled": True,
    "fault_tolerance": {"max_retries": 3}
})
```

---

## Essential Operations

### 1. Basic Pipeline Creation

**Simple data processing pipeline:**

```python
from xlibrary.pipeline import PipelineManager, Pipeline, Task

pm = PipelineManager()

# Create pipeline
data_pipeline = Pipeline("data_processing")

# Define tasks
@data_pipeline.task("extract")
def extract_data():
    """Extract data from source."""
    return {"data": [1, 2, 3, 4, 5], "timestamp": "2024-01-01"}

@data_pipeline.task("transform", depends_on=["extract"])
def transform_data(extract_result):
    """Transform extracted data."""
    data = extract_result["data"]
    transformed = [x * 2 for x in data]
    return {"transformed_data": transformed}

@data_pipeline.task("load", depends_on=["transform"])
def load_data(transform_result):
    """Load transformed data."""
    print(f"Loading: {transform_result['transformed_data']}")
    return {"status": "loaded", "count": len(transform_result["transformed_data"])}

# Execute pipeline
result = pm.execute_pipeline(data_pipeline)
print(f"Pipeline completed: {result.success}")
print(f"Final result: {result.final_result}")
```

### 2. Parallel Task Execution

**Tasks that can run in parallel:**

```python
# Create pipeline with parallel tasks
ml_pipeline = Pipeline("machine_learning")

@ml_pipeline.task("prepare_data")
def prepare_data():
    """Prepare training data."""
    return {"features": [[1, 2], [3, 4]], "labels": [0, 1]}

# These tasks can run in parallel
@ml_pipeline.task("train_model_a", depends_on=["prepare_data"])
def train_random_forest(data_result):
    """Train random forest model."""
    print("Training Random Forest...")
    time.sleep(2)  # Simulate training
    return {"model": "random_forest", "accuracy": 0.85}

@ml_pipeline.task("train_model_b", depends_on=["prepare_data"])
def train_svm(data_result):
    """Train SVM model."""
    print("Training SVM...")
    time.sleep(3)  # Simulate training
    return {"model": "svm", "accuracy": 0.82}

@ml_pipeline.task("train_model_c", depends_on=["prepare_data"])
def train_neural_net(data_result):
    """Train neural network."""
    print("Training Neural Network...")
    time.sleep(4)  # Simulate training
    return {"model": "neural_net", "accuracy": 0.88}

@ml_pipeline.task("select_best_model", depends_on=["train_model_a", "train_model_b", "train_model_c"])
def select_best_model(model_a, model_b, model_c):
    """Select best performing model."""
    models = [model_a, model_b, model_c]
    best = max(models, key=lambda x: x["accuracy"])
    return {"best_model": best["model"], "accuracy": best["accuracy"]}

# Execute with parallel processing
result = pm.execute_pipeline(ml_pipeline, parallel=True)
print(f"Best model: {result.final_result['best_model']}")
```

### 3. Stream Processing

**Real-time data stream processing:**

```python
from xlibrary.pipeline import StreamPipeline, StreamTask

# Create streaming pipeline
stream_pipeline = StreamPipeline("real_time_analytics")

class DataProcessor(StreamTask):
    def process_batch(self, batch):
        """Process batch of streaming data."""
        processed = []
        for item in batch:
            # Example: calculate running average
            processed.append({
                "id": item.get("id"),
                "value": item.get("value", 0) * 1.1,
                "processed_at": time.time()
            })
        return processed

class Aggregator(StreamTask):
    def __init__(self):
        super().__init__()
        self.running_sum = 0
        self.count = 0

    def process_batch(self, batch):
        """Aggregate streaming data."""
        for item in batch:
            self.running_sum += item.get("value", 0)
            self.count += 1

        avg = self.running_sum / self.count if self.count > 0 else 0
        return [{"average": avg, "count": self.count}]

# Configure stream sources
stream_pipeline.add_source("data_stream", {
    "type": "kafka",
    "topic": "sensor_data",
    "batch_size": 100
})

# Add processing stages
stream_pipeline.add_stage(DataProcessor(), parallelism=2)
stream_pipeline.add_stage(Aggregator(), parallelism=1)

# Add output sink
stream_pipeline.add_sink("results", {
    "type": "database",
    "connection": "postgresql://localhost/analytics"
})

# Start stream processing
stream_pipeline.start()
```

### 4. Distributed Execution

**Run pipelines across multiple nodes:**

```python
# Configure distributed execution
distributed_config = {
    "cluster_mode": "kubernetes",
    "worker_nodes": 3,
    "resource_allocation": {
        "cpu_per_worker": 2,
        "memory_per_worker": "4Gi"
    }
}

distributed_pm = PipelineManager(distributed_config)

# Create large-scale processing pipeline
big_data_pipeline = Pipeline("large_scale_etl")

@big_data_pipeline.task("partition_data")
def partition_large_dataset():
    """Partition large dataset for distributed processing."""
    # Simulate large dataset partitioning
    partitions = [{"partition_id": i, "data_size": "1GB"} for i in range(10)]
    return {"partitions": partitions}

@big_data_pipeline.task("process_partitions", depends_on=["partition_data"])
def process_partitions(partition_result):
    """Process data partitions in parallel across nodes."""
    partitions = partition_result["partitions"]

    # This will be distributed across worker nodes
    results = []
    for partition in partitions:
        # Simulate partition processing
        processed = {
            "partition_id": partition["partition_id"],
            "processed_records": 1000000,
            "processing_time": 120
        }
        results.append(processed)

    return {"processed_partitions": results}

@big_data_pipeline.task("aggregate_results", depends_on=["process_partitions"])
def aggregate_results(processing_result):
    """Aggregate results from all partitions."""
    partitions = processing_result["processed_partitions"]
    total_records = sum(p["processed_records"] for p in partitions)
    total_time = max(p["processing_time"] for p in partitions)

    return {
        "total_records_processed": total_records,
        "total_processing_time": total_time,
        "throughput": total_records / total_time
    }

# Execute on distributed cluster
result = distributed_pm.execute_distributed(big_data_pipeline)
print(f"Processed {result.final_result['total_records_processed']} records")
```

---

## Common Use Cases

### Data Pipeline

**Complete ETL pipeline with error handling:**

```python
def create_etl_pipeline():
    """Create robust ETL pipeline."""

    etl_pipeline = Pipeline("customer_etl")

    @etl_pipeline.task("extract_customers")
    @etl_pipeline.retry_on_failure(max_retries=3)
    def extract_customer_data():
        """Extract customer data from multiple sources."""
        try:
            # Simulate database extraction
            customers = [
                {"id": 1, "name": "John Doe", "email": "john@example.com"},
                {"id": 2, "name": "Jane Smith", "email": "jane@example.com"}
            ]
            return {"customers": customers, "extracted_at": time.time()}
        except Exception as e:
            raise ExtractionError(f"Failed to extract customers: {str(e)}")

    @etl_pipeline.task("validate_data", depends_on=["extract_customers"])
    def validate_customer_data(extract_result):
        """Validate extracted customer data."""
        customers = extract_result["customers"]
        valid_customers = []

        for customer in customers:
            if customer.get("email") and "@" in customer["email"]:
                valid_customers.append(customer)

        return {"valid_customers": valid_customers, "validated_at": time.time()}

    @etl_pipeline.task("enrich_data", depends_on=["validate_data"])
    def enrich_customer_data(validation_result):
        """Enrich customer data with additional information."""
        customers = validation_result["valid_customers"]

        for customer in customers:
            # Simulate enrichment
            customer["segment"] = "premium" if customer["id"] % 2 == 0 else "standard"
            customer["enriched_at"] = time.time()

        return {"enriched_customers": customers}

    @etl_pipeline.task("load_data", depends_on=["enrich_data"])
    def load_customer_data(enrichment_result):
        """Load enriched data to data warehouse."""
        customers = enrichment_result["enriched_customers"]

        # Simulate loading to warehouse
        print(f"Loading {len(customers)} customers to data warehouse...")

        return {
            "loaded_customers": len(customers),
            "load_completed_at": time.time(),
            "status": "success"
        }

    return etl_pipeline

# Execute ETL pipeline
etl_pipeline = create_etl_pipeline()
result = pm.execute_pipeline(etl_pipeline)

if result.success:
    print("ETL pipeline completed successfully!")
    print(f"Loaded {result.final_result['loaded_customers']} customers")
else:
    print(f"ETL pipeline failed: {result.error}")
```

### ML Training Pipeline

**Machine learning model training with validation:**

```python
def create_ml_training_pipeline():
    """Create ML model training pipeline."""

    ml_pipeline = Pipeline("ml_model_training")

    @ml_pipeline.task("load_dataset")
    def load_training_data():
        """Load and prepare training dataset."""
        # Simulate loading dataset
        import numpy as np

        X = np.random.rand(1000, 10)  # 1000 samples, 10 features
        y = np.random.randint(0, 2, 1000)  # Binary classification

        return {"X_train": X, "y_train": y, "samples": 1000, "features": 10}

    @ml_pipeline.task("feature_engineering", depends_on=["load_dataset"])
    def engineer_features(data_result):
        """Engineer features for model training."""
        X = data_result["X_train"]

        # Simulate feature engineering
        X_engineered = X * 2 + 0.1  # Simple transformation

        return {
            "X_engineered": X_engineered,
            "y_train": data_result["y_train"],
            "feature_names": [f"feature_{i}" for i in range(X.shape[1])]
        }

    @ml_pipeline.task("train_model", depends_on=["feature_engineering"])
    def train_model(feature_result):
        """Train machine learning model."""
        X = feature_result["X_engineered"]
        y = feature_result["y_train"]

        # Simulate model training
        time.sleep(2)  # Simulate training time

        # Mock model performance
        accuracy = np.random.uniform(0.8, 0.95)

        return {
            "model": "trained_model_object",
            "accuracy": accuracy,
            "training_completed_at": time.time()
        }

    @ml_pipeline.task("validate_model", depends_on=["train_model"])
    def validate_model(training_result):
        """Validate trained model."""
        accuracy = training_result["accuracy"]

        # Validation logic
        is_valid = accuracy >= 0.85

        return {
            "model": training_result["model"],
            "validation_accuracy": accuracy,
            "is_valid": is_valid,
            "validation_completed_at": time.time()
        }

    @ml_pipeline.task("deploy_model", depends_on=["validate_model"])
    def deploy_model(validation_result):
        """Deploy model if validation passes."""
        if not validation_result["is_valid"]:
            raise ValidationError("Model failed validation - not deploying")

        # Simulate deployment
        print("Deploying model to production...")

        return {
            "deployed": True,
            "model_version": "v1.0",
            "deployment_endpoint": "https://api.example.com/predict",
            "deployed_at": time.time()
        }

    return ml_pipeline

# Execute ML pipeline
ml_pipeline = create_ml_training_pipeline()
result = pm.execute_pipeline(ml_pipeline)

if result.success:
    print("ML pipeline completed successfully!")
    final_result = result.final_result
    print(f"Model deployed: {final_result.get('deployed', False)}")
    print(f"Endpoint: {final_result.get('deployment_endpoint', 'N/A')}")
```

### Batch Processing Pipeline

**Large-scale batch processing with monitoring:**

```python
def create_batch_processing_pipeline():
    """Create batch processing pipeline with monitoring."""

    batch_pipeline = Pipeline("daily_batch_processing")

    @batch_pipeline.task("initialize_batch")
    @batch_pipeline.monitor_performance
    def initialize_batch_job():
        """Initialize daily batch processing job."""
        batch_id = f"batch_{int(time.time())}"
        batch_date = time.strftime("%Y-%m-%d")

        return {
            "batch_id": batch_id,
            "batch_date": batch_date,
            "initialized_at": time.time()
        }

    @batch_pipeline.task("process_orders", depends_on=["initialize_batch"])
    @batch_pipeline.monitor_performance
    def process_daily_orders(batch_info):
        """Process daily orders batch."""
        batch_id = batch_info["batch_id"]

        # Simulate processing orders
        orders_processed = 0
        for i in range(10000):  # Process 10k orders
            # Simulate order processing
            orders_processed += 1

            if i % 1000 == 0:
                print(f"Processed {i} orders...")

        return {
            "batch_id": batch_id,
            "orders_processed": orders_processed,
            "processing_completed_at": time.time()
        }

    @batch_pipeline.task("generate_reports", depends_on=["process_orders"])
    @batch_pipeline.monitor_performance
    def generate_daily_reports(processing_result):
        """Generate daily reports."""
        batch_id = processing_result["batch_id"]
        orders_count = processing_result["orders_processed"]

        # Simulate report generation
        reports = [
            {"name": "sales_summary", "orders": orders_count},
            {"name": "customer_analysis", "customers": orders_count // 2},
            {"name": "inventory_update", "items": orders_count * 3}
        ]

        return {
            "batch_id": batch_id,
            "reports_generated": reports,
            "reports_completed_at": time.time()
        }

    @batch_pipeline.task("send_notifications", depends_on=["generate_reports"])
    def send_completion_notifications(reports_result):
        """Send batch completion notifications."""
        batch_id = reports_result["batch_id"]
        reports = reports_result["reports_generated"]

        # Simulate sending notifications
        notifications_sent = []
        stakeholders = ["manager@company.com", "analyst@company.com"]

        for stakeholder in stakeholders:
            notification = {
                "recipient": stakeholder,
                "subject": f"Daily Batch {batch_id} Completed",
                "reports_count": len(reports),
                "sent_at": time.time()
            }
            notifications_sent.append(notification)

        return {
            "batch_id": batch_id,
            "notifications_sent": notifications_sent,
            "batch_completed_at": time.time()
        }

    return batch_pipeline

# Execute with monitoring
batch_pipeline = create_batch_processing_pipeline()

# Enable monitoring
pm.enable_monitoring(
    metrics=["execution_time", "memory_usage", "task_status"],
    alerts=["task_failure", "long_execution"],
    dashboard=True
)

result = pm.execute_pipeline(batch_pipeline)

# Get performance metrics
performance_report = pm.get_performance_report(batch_pipeline.name)
print(f"Total execution time: {performance_report['total_time']:.2f}s")
print(f"Memory peak: {performance_report['peak_memory']} MB")
```

---

## Performance Optimization

### Resource Management

**Optimize resource allocation:**

```python
# Configure resource optimization
optimized_pm = PipelineManager({
    "resource_optimization": True,
    "auto_scaling": True,
    "resource_limits": {
        "max_cpu_cores": 8,
        "max_memory_gb": 16,
        "optimization_strategy": "balanced"
    }
})

# Resource-aware pipeline
@optimized_pm.pipeline("resource_optimized")
def resource_optimized_pipeline():

    @task("cpu_intensive", resource_requirements={"cpu_cores": 4})
    def cpu_heavy_task():
        """CPU intensive computation."""
        # Simulate CPU heavy work
        result = sum(i**2 for i in range(1000000))
        return {"computation_result": result}

    @task("memory_intensive", resource_requirements={"memory_mb": 4096})
    def memory_heavy_task():
        """Memory intensive operation."""
        # Simulate memory intensive work
        large_list = list(range(1000000))
        return {"data_size": len(large_list)}

    @task("io_intensive", resource_requirements={"cpu_cores": 1, "memory_mb": 512})
    def io_heavy_task():
        """I/O intensive operation."""
        # Simulate I/O work
        time.sleep(2)
        return {"io_operations": 1000}

# Execute with resource optimization
result = optimized_pm.execute_pipeline(resource_optimized_pipeline)
```

### Caching and Optimization

**Enable intelligent caching:**

```python
# Configure caching
cached_pm = PipelineManager({
    "caching": {
        "enabled": True,
        "cache_type": "redis",
        "ttl": 3600,  # 1 hour
        "cache_results": True
    }
})

@cached_pm.pipeline("cached_pipeline")
def create_cached_pipeline():

    @task("expensive_computation")
    @cached_pm.cache_result(key="daily_computation", ttl=86400)
    def expensive_daily_computation():
        """Expensive computation that should be cached."""
        print("Performing expensive computation...")
        time.sleep(5)  # Simulate expensive work
        return {"result": "computed_value", "computed_at": time.time()}

    @task("use_cached_result", depends_on=["expensive_computation"])
    def use_cached_result(computation_result):
        """Use cached computation result."""
        return {"used_cached": True, "value": computation_result["result"]}

# First execution - will compute
result1 = cached_pm.execute_pipeline(create_cached_pipeline)

# Second execution - will use cache
result2 = cached_pm.execute_pipeline(create_cached_pipeline)
```

---

## Monitoring and Debugging

### Real-time Monitoring

**Monitor pipeline execution:**

```python
# Enable comprehensive monitoring
monitoring_pm = PipelineManager({
    "monitoring": {
        "real_time": True,
        "metrics": ["cpu", "memory", "task_duration", "queue_depth"],
        "alerts": {
            "task_timeout": 300,  # 5 minutes
            "memory_threshold": 0.9,
            "error_rate_threshold": 0.1
        }
    }
})

# Create monitored pipeline
monitored_pipeline = Pipeline("monitored_etl")

@monitored_pipeline.task("data_extraction")
@monitoring_pm.track_performance
def extract_with_monitoring():
    """Extract data with performance tracking."""
    start_time = time.time()

    # Simulate data extraction
    data = {"records": 10000}
    time.sleep(1)

    processing_time = time.time() - start_time
    monitoring_pm.log_metric("extraction_time", processing_time)
    monitoring_pm.log_metric("records_extracted", data["records"])

    return data

# Execute with real-time monitoring
result = monitoring_pm.execute_pipeline(monitored_pipeline)

# Get monitoring dashboard
dashboard = monitoring_pm.get_monitoring_dashboard()
print(f"Current metrics: {dashboard.get_current_metrics()}")
print(f"Performance trends: {dashboard.get_performance_trends()}")
```

### Debugging and Troubleshooting

**Debug pipeline issues:**

```python
# Enable debugging
debug_pm = PipelineManager({
    "debug_mode": True,
    "detailed_logging": True,
    "save_intermediate_results": True
})

try:
    result = debug_pm.execute_pipeline(problematic_pipeline)
except PipelineExecutionError as e:
    # Get detailed error information
    error_details = debug_pm.get_error_details(e)

    print(f"Pipeline failed at task: {error_details.failed_task}")
    print(f"Error message: {error_details.error_message}")
    print(f"Stack trace: {error_details.stack_trace}")

    # Get intermediate results for debugging
    intermediate_results = debug_pm.get_intermediate_results(problematic_pipeline.name)
    print(f"Completed tasks: {list(intermediate_results.keys())}")

    # Retry specific task
    debug_pm.retry_failed_task(
        pipeline_name=problematic_pipeline.name,
        task_name=error_details.failed_task
    )
```

---

## Integration Examples

### FastAPI Integration

**Web API with pipeline execution:**

```python
from fastapi import FastAPI, BackgroundTasks
from xlibrary.pipeline import PipelineManager

app = FastAPI()
pm = PipelineManager()

@app.post("/process-data")
async def process_data_endpoint(data: dict, background_tasks: BackgroundTasks):
    """Trigger data processing pipeline via API."""

    # Create dynamic pipeline
    api_pipeline = Pipeline("api_data_processing")

    @api_pipeline.task("process_request")
    def process_api_request():
        """Process API request data."""
        # Process the incoming data
        processed_data = {
            "original_data": data,
            "processed_at": time.time(),
            "status": "processed"
        }
        return processed_data

    # Execute pipeline in background
    def run_pipeline():
        result = pm.execute_pipeline(api_pipeline)
        # Store result or send notification

    background_tasks.add_task(run_pipeline)

    return {"message": "Processing started", "pipeline_id": api_pipeline.name}

@app.get("/pipeline-status/{pipeline_id}")
async def get_pipeline_status(pipeline_id: str):
    """Get pipeline execution status."""
    status = pm.get_pipeline_status(pipeline_id)
    return {"pipeline_id": pipeline_id, "status": status}
```

### Jupyter Notebook Integration

**Interactive pipeline development:**

```python
# Notebook-friendly pipeline creation
from xlibrary.pipeline import PipelineManager

# Configure for notebook use
notebook_pm = PipelineManager({
    "interactive_mode": True,
    "display_progress": True,
    "inline_results": True
})

# Create and test pipeline interactively
data_analysis_pipeline = Pipeline("data_analysis")

@data_analysis_pipeline.task("load_data")
def load_notebook_data():
    """Load data for analysis."""
    import pandas as pd
    # Create sample data
    df = pd.DataFrame({
        "x": range(100),
        "y": [i**2 for i in range(100)]
    })
    return {"dataframe": df, "shape": df.shape}

@data_analysis_pipeline.task("analyze", depends_on=["load_data"])
def analyze_data(data_result):
    """Analyze loaded data."""
    df = data_result["dataframe"]

    analysis = {
        "mean_x": df["x"].mean(),
        "mean_y": df["y"].mean(),
        "correlation": df["x"].corr(df["y"])
    }

    return analysis

# Execute with progress display in notebook
result = notebook_pm.execute_pipeline(
    data_analysis_pipeline,
    display_progress=True,
    show_intermediate=True
)

# Display results inline
result.display_results()
```

---

## Next Steps

Ready to dive deeper? Continue with:

- **[Detailed User Guide](09.04%20Chapter%209%20-%20Pipeline%20Manager%20-%20User%20Guide%20-%20Detailed.md)** - Advanced patterns, enterprise workflows, and complex scenarios
- **[Chapter 10: CLI Framework](10.01%20Chapter%2010%20-%20CLI%20Framework%20-%20Design%20-%20Overview.md)** - Command-line interface development framework

**You now have the essential tools to orchestrate sophisticated data processing pipelines with enterprise-grade reliability!** ⚡