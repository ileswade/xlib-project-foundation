# Chapter 5: Media Manager - User Guide Detailed

> **ðŸŽ¬ COMPREHENSIVE MEDIA PROCESSING MASTERY**
> Complete guide to xlibrary's Media Manager: advanced video editing, sophisticated watermarking systems, professional animation workflows, and enterprise media processing pipelines.

## Advanced Video Processing Workflows

### Professional Video Editing Suite

**What this example demonstrates:** How to build a complete professional video editing workflow with frame-accurate precision, automated processing, and broadcast-quality output. This approach is suitable for content creators, video production companies, and automated media processing pipelines.

**Key concepts to notice:**
- Professional-grade configuration (CRF 18 for high quality, hardware acceleration)
- Frame-accurate trimming and timing control
- Automated segment processing with effects pipeline
- Seamless concatenation with transitions
- Quality-preserving processing throughout the workflow
- Comprehensive error handling and cleanup

**How the editing pipeline works:**
1. **Analysis Phase**: Examines all source clips for resolution, duration, and technical properties
2. **Segmentation**: Precisely trims clips according to edit list specifications
3. **Effects Application**: Applies segment-specific effects (color correction, filters, etc.)
4. **Assembly**: Concatenates segments with smooth transitions
5. **Final Processing**: Applies global corrections and optimizations
6. **Cleanup**: Removes temporary files while preserving the final output

**When to use this:** Automated video production, content creation workflows, educational video generation, or any scenario requiring professional-quality video editing with programmatic control.

The Media Manager provides professional-grade video editing capabilities with frame-accurate control:

```python
from xlibrary.media import MediaManager, ProcessingConfig, WatermarkConfig
from xlibrary.media.utils import parse_timestamp, seconds_to_hms
from pathlib import Path

class ProfessionalVideoEditor:
    def __init__(self):
        # Configure for professional quality
        config = ProcessingConfig(
            video_codec="h264",
            quality_crf=18,               # Professional quality (18-23 range)
            audio_codec="aac",
            audio_bitrate="256k",
            preserve_metadata=True,
            hardware_acceleration=True
        )

        self.mm = MediaManager(config=config)
        self.temp_files = []

    def create_professional_edit(self, source_clips, edit_list, output_path):
        """
        Create professional video edit with precise timing and transitions.

        Args:
            source_clips: List of source video files
            edit_list: List of edit instructions with timing
            output_path: Final output video path
        """

        print("ðŸŽ¬ Starting professional video edit...")

        # Step 1: Analyze all source clips
        clip_analyses = {}
        for clip_path in source_clips:
            analysis = self.mm.analyze_video(clip_path)
            clip_analyses[clip_path] = analysis
            print(f"ðŸ“Š {Path(clip_path).name}: {analysis.resolution}, {analysis.duration:.1f}s")

        # Step 2: Process edit list
        edited_segments = []

        for i, edit in enumerate(edit_list):
            source_file = edit['source']
            start_time = edit['start']
            end_time = edit['end']

            print(f"âœ‚ï¸ Processing segment {i+1}: {start_time} to {end_time}")

            # Frame-accurate trimming
            segment = self.mm.trim_video(
                source_file,
                start_time=start_time,
                end_time=end_time,
                output_path=f"temp_segment_{i:03d}.mp4",
                preserve_quality=True
            )

            # Apply segment-specific effects if specified
            if 'effects' in edit:
                segment = self._apply_segment_effects(segment.output_path, edit['effects'])

            edited_segments.append(segment.output_path)
            self.temp_files.append(segment.output_path)

        # Step 3: Concatenate segments with transitions
        print("ðŸ”— Concatenating segments...")
        final_edit = self.mm.concatenate_videos(
            edited_segments,
            output_path=output_path,
            add_transitions=True,
            transition_duration=0.5
        )

        # Step 4: Apply final color correction and audio processing
        if final_edit.success:
            final_result = self._apply_final_processing(
                final_edit.output_path,
                output_path.replace('.mp4', '_final.mp4')
            )

        # Cleanup temporary files
        self._cleanup_temp_files()

        return final_result

    def _apply_segment_effects(self, segment_path, effects):
        """Apply effects to individual segment."""
        working_path = segment_path

        for effect in effects:
            if effect['type'] == 'speed':
                # Speed adjustment
                speed_result = self.mm.adjust_speed(
                    working_path,
                    speed_factor=effect['factor'],
                    output_path=working_path.replace('.mp4', f"_speed{effect['factor']}.mp4")
                )
                working_path = speed_result.output_path
                self.temp_files.append(working_path)

            elif effect['type'] == 'fade_in':
                # Add fade in effect
                fade_result = self.mm.apply_fade(
                    working_path,
                    fade_type='in',
                    duration=effect['duration'],
                    output_path=working_path.replace('.mp4', '_fade_in.mp4')
                )
                working_path = fade_result.output_path
                self.temp_files.append(working_path)

            elif effect['type'] == 'fade_out':
                # Add fade out effect
                fade_result = self.mm.apply_fade(
                    working_path,
                    fade_type='out',
                    duration=effect['duration'],
                    output_path=working_path.replace('.mp4', '_fade_out.mp4')
                )
                working_path = fade_result.output_path
                self.temp_files.append(working_path)

        return type('Result', (), {'output_path': working_path})()

    def _apply_final_processing(self, video_path, output_path):
        """Apply final professional processing."""
        # Color correction, audio normalization, etc.
        return self.mm.apply_professional_finish(
            video_path,
            output_path=output_path,
            color_correction=True,
            audio_normalization=True,
            noise_reduction=True
        )

    def _cleanup_temp_files(self):
        """Clean up temporary files."""
        for temp_file in self.temp_files:
            if Path(temp_file).exists():
                Path(temp_file).unlink()
        self.temp_files.clear()

# Usage example
editor = ProfessionalVideoEditor()

source_clips = ["interview.mp4", "broll_1.mp4", "broll_2.mp4"]

edit_sequence = [
    {
        'source': 'interview.mp4',
        'start': '0:05',
        'end': '0:45',
        'effects': [{'type': 'fade_in', 'duration': 1.0}]
    },
    {
        'source': 'broll_1.mp4',
        'start': '0:10',
        'end': '0:25',
        'effects': []
    },
    {
        'source': 'interview.mp4',
        'start': '0:45',
        'end': '2:30',
        'effects': []
    },
    {
        'source': 'broll_2.mp4',
        'start': '0:00',
        'end': '0:15',
        'effects': [{'type': 'fade_out', 'duration': 1.0}]
    }
]

final_video = editor.create_professional_edit(
    source_clips,
    edit_sequence,
    "professional_edit.mp4"
)
```

### Advanced Watermarking System

**What this example demonstrates:** How to create sophisticated, multi-phase watermarking campaigns with dynamic positioning, animations, and time-based branding strategies. This approach is essential for content creators, broadcasters, and organizations that need professional branding throughout their video content.

**Key concepts to notice:**
- Multi-phase watermarking with different brand elements over time
- Dynamic animations (fade in/out, sliding, pulsing effects)
- Precise timing control for watermark appearance and behavior
- Professional styling with shadows, opacity, and sizing controls
- Campaign-based approach for consistent branding across content
- Conflict detection to avoid overlapping watermarks

**How the watermarking system works:**
1. **Campaign Planning**: Defines multiple watermark phases with timing and positioning
2. **Brand Introduction**: Shows prominent logo during opening seconds
3. **Persistent Branding**: Maintains subtle branding throughout content
4. **Strategic Positioning**: Uses intelligent positioning to avoid content conflicts
5. **Animation System**: Smoothly transitions watermarks in and out
6. **Quality Optimization**: Ensures watermarks enhance rather than detract from content

**When to use this:** Brand protection, content monetization, professional video production, social media content, or any scenario requiring sophisticated visual branding.

Sophisticated watermarking with dynamic positioning and complex animations:

```python
from xlibrary.media import MediaManager, WatermarkConfig, WatermarkAnimation, WatermarkPosition
from xlibrary.media.core.types import ShadowConfig, Margin

class AdvancedWatermarkSystem:
    def __init__(self):
        self.mm = MediaManager()

    def create_dynamic_watermark_campaign(self, video_path, campaign_config):
        """Create sophisticated watermark campaign with multiple phases."""

        # Phase 1: Brand introduction (first 10 seconds)
        intro_config = WatermarkConfig(
            watermark_path=campaign_config['brand_logo'],
            position=WatermarkPosition.CENTER,
            animation=WatermarkAnimation.FADE_IN,
            animation_duration=3.0,
            animation_start_time=2.0,
            animation_end_time=10.0,
            max_width_percent=0.3,
            opacity=0.95,
            shadow=ShadowConfig(
                enabled=True,
                blur=5,
                offset=(3, 3),
                color="black",
                opacity=0.6
            )
        )

        # Phase 2: Persistent branding (throughout video)
        persistent_config = WatermarkConfig(
            watermark_path=campaign_config['small_logo'],
            position=WatermarkPosition.BOTTOM_RIGHT,
            animation=WatermarkAnimation.STATIC,
            animation_start_time=10.0,
            max_width_percent=0.12,
            opacity=0.7,
            margin=Margin(20, 20, 20, 20)
        )

        # Phase 3: Call-to-action (last 15 seconds)
        video_duration = self.mm.get_video_duration(video_path)
        cta_start = max(0, video_duration - 15)

        cta_config = WatermarkConfig(
            watermark_path=campaign_config['cta_graphic'],
            position=WatermarkPosition.BOTTOM_CENTER,
            animation=WatermarkAnimation.PULSE,
            animation_start_time=cta_start,
            animation_duration=2.0,  # 2-second pulse cycle
            max_width_percent=0.4,
            opacity=0.9,
            pulse_amplitude=0.15,    # 15% size variation
            pulse_frequency=0.5      # Pulse every 2 seconds
        )

        # Apply all watermark phases
        print("ðŸŽ¨ Applying brand introduction...")
        step1 = self.mm.watermark_video(video_path, intro_config, "temp_step1.mp4")

        print("ðŸ·ï¸ Adding persistent branding...")
        step2 = self.mm.watermark_video(step1.output_path, persistent_config, "temp_step2.mp4")

        print("ðŸ“¢ Adding call-to-action...")
        final_result = self.mm.watermark_video(step2.output_path, cta_config, "branded_campaign.mp4")

        # Cleanup temporary files
        Path(step1.output_path).unlink()
        Path(step2.output_path).unlink()

        return final_result

    def create_adaptive_watermark_series(self, video_list, brand_assets):
        """Create series with adaptive watermarking based on content."""

        results = []

        for i, video_path in enumerate(video_list):
            print(f"ðŸŽ¬ Processing video {i+1}/{len(video_list)}: {Path(video_path).name}")

            # Analyze video content for optimal watermark placement
            analysis = self.mm.analyze_video(video_path)
            scene_analysis = self.mm.analyze_scenes(video_path)

            # Choose watermark strategy based on content
            if scene_analysis.has_dark_scenes:
                # Use light watermark for dark content
                watermark_path = brand_assets['light_logo']
                opacity = 0.9
            else:
                # Use dark watermark for light content
                watermark_path = brand_assets['dark_logo']
                opacity = 0.7

            # Adaptive positioning based on scene composition
            if scene_analysis.has_bottom_text:
                position = WatermarkPosition.TOP_RIGHT
            else:
                position = WatermarkPosition.BOTTOM_RIGHT

            # Create adaptive configuration
            adaptive_config = WatermarkConfig(
                watermark_path=watermark_path,
                position=position,
                opacity=opacity,
                max_width_percent=0.15 if analysis.resolution.width >= 1920 else 0.20,
                auto_scale=True,

                # Smart animation based on video length
                animation=WatermarkAnimation.FADE_IN_OUT if analysis.duration < 60 else WatermarkAnimation.STATIC,
                animation_duration=5.0 if analysis.duration < 60 else 0.0
            )

            # Apply watermark
            result = self.mm.watermark_video(
                video_path,
                adaptive_config,
                f"adaptive_branded_{i+1:02d}_{Path(video_path).stem}.mp4"
            )

            results.append({
                'original': video_path,
                'branded': result.output_path,
                'watermark_used': watermark_path,
                'position': position.value,
                'processing_time': result.processing_time
            })

        return results

# Usage
watermark_system = AdvancedWatermarkSystem()

# Dynamic campaign example
campaign_assets = {
    'brand_logo': 'brand_intro_logo.png',
    'small_logo': 'persistent_logo.png',
    'cta_graphic': 'subscribe_now.png'
}

campaign_result = watermark_system.create_dynamic_watermark_campaign(
    "content_video.mp4",
    campaign_assets
)

# Adaptive series example
brand_assets = {
    'light_logo': 'logo_white.png',
    'dark_logo': 'logo_black.png'
}

video_series = ["episode_01.mp4", "episode_02.mp4", "episode_03.mp4"]

series_results = watermark_system.create_adaptive_watermark_series(
    video_series,
    brand_assets
)

for result in series_results:
    print(f"âœ… {result['original']} -> {result['branded']}")
    print(f"   Watermark: {Path(result['watermark_used']).name}, Position: {result['position']}")
```

### Complex Animation Workflows

Advanced animation systems for professional video production:

```python
from xlibrary.media import MediaManager, WatermarkConfig, WatermarkAnimation
from xlibrary.media.animation import AnimationEngine, AnimationSequence

class ComplexAnimationProducer:
    def __init__(self):
        self.mm = MediaManager()
        self.animation_engine = AnimationEngine()

    def create_kinetic_typography(self, text_elements, background_video, output_path):
        """Create kinetic typography animation over video background."""

        print("ðŸ“ Creating kinetic typography animation...")

        # Generate text animation frames
        text_animations = []

        for i, text_element in enumerate(text_elements):
            # Create text graphic
            text_image = self._create_text_graphic(
                text_element['text'],
                text_element.get('font_size', 72),
                text_element.get('color', 'white'),
                text_element.get('font_family', 'Arial Bold')
            )

            # Define animation timing
            start_time = text_element.get('start_time', i * 3.0)
            duration = text_element.get('duration', 5.0)
            end_time = start_time + duration

            # Create complex animation config
            text_config = WatermarkConfig(
                watermark_path=text_image,
                position=text_element.get('position', WatermarkPosition.CENTER),
                animation=text_element.get('animation', WatermarkAnimation.FADE_IN_OUT),
                animation_start_time=start_time,
                animation_end_time=end_time,
                animation_duration=duration,
                max_width_percent=text_element.get('scale', 0.6),
                opacity=text_element.get('opacity', 0.95)
            )

            # Add custom animation properties
            if text_element.get('animation') == 'slide_in_left':
                text_config.custom_animation = self._create_slide_in_animation(
                    direction='left',
                    duration=duration * 0.3  # 30% of total duration for slide
                )

            text_animations.append(text_config)

        # Apply animations sequentially
        working_video = background_video
        temp_files = []

        for i, config in enumerate(text_animations):
            print(f"ðŸ“‹ Applying text animation {i+1}/{len(text_animations)}...")

            temp_output = f"temp_text_{i:02d}.mp4"
            result = self.mm.watermark_video(working_video, config, temp_output)

            working_video = result.output_path
            temp_files.append(temp_output)

        # Final output
        final_result = self.mm.finalize_video(
            working_video,
            output_path=output_path,
            optimize=True
        )

        # Cleanup
        for temp_file in temp_files:
            if Path(temp_file).exists():
                Path(temp_file).unlink()

        return final_result

    def create_multi_logo_sequence(self, video_path, logo_sequence, output_path):
        """Create complex multi-logo animation sequence."""

        print("ðŸ¢ Creating multi-logo sequence...")

        # Design logo transition timeline
        timeline_events = []

        for i, logo_event in enumerate(logo_sequence):
            start_time = logo_event.get('start_time', i * 8.0)

            # Logo introduction phase
            intro_config = WatermarkConfig(
                watermark_path=logo_event['logo_path'],
                position=logo_event.get('position', WatermarkPosition.CENTER),
                animation=WatermarkAnimation.FADE_IN,
                animation_start_time=start_time,
                animation_duration=2.0,
                max_width_percent=logo_event.get('scale', 0.25),
                opacity=0.95
            )

            # Logo hold phase
            hold_config = WatermarkConfig(
                watermark_path=logo_event['logo_path'],
                position=logo_event.get('position', WatermarkPosition.CENTER),
                animation=WatermarkAnimation.STATIC,
                animation_start_time=start_time + 2.0,
                animation_end_time=start_time + 6.0,
                max_width_percent=logo_event.get('scale', 0.25),
                opacity=0.95
            )

            # Logo exit phase
            exit_config = WatermarkConfig(
                watermark_path=logo_event['logo_path'],
                position=logo_event.get('position', WatermarkPosition.CENTER),
                animation=WatermarkAnimation.FADE_OUT,
                animation_start_time=start_time + 6.0,
                animation_duration=2.0,
                max_width_percent=logo_event.get('scale', 0.25),
                opacity=0.95
            )

            timeline_events.extend([intro_config, hold_config, exit_config])

        # Apply all timeline events
        working_video = video_path

        for i, config in enumerate(timeline_events):
            temp_output = f"temp_timeline_{i:03d}.mp4"
            result = self.mm.watermark_video(working_video, config, temp_output)

            if i > 0 and Path(working_video).name.startswith('temp_'):
                Path(working_video).unlink()  # Cleanup previous temp file

            working_video = result.output_path

        # Finalize
        final_result = shutil.move(working_video, output_path)

        return type('Result', (), {
            'success': True,
            'output_path': output_path,
            'timeline_events': len(timeline_events)
        })()

    def create_animated_infographics(self, data_points, template_video, output_path):
        """Create animated infographics overlay on video."""

        print("ðŸ“Š Creating animated infographics...")

        infographic_elements = []

        for i, data_point in enumerate(data_points):
            # Create infographic element
            infographic = self._create_infographic_element(
                title=data_point['title'],
                value=data_point['value'],
                chart_type=data_point.get('chart_type', 'bar'),
                color_scheme=data_point.get('colors', 'blue')
            )

            # Animation timing
            start_time = data_point.get('start_time', i * 4.0)
            duration = data_point.get('duration', 6.0)

            # Create animated reveal
            reveal_config = WatermarkConfig(
                watermark_path=infographic,
                position=data_point.get('position', WatermarkPosition.CENTER_LEFT),
                animation=WatermarkAnimation.SLIDE_IN_LEFT,
                animation_start_time=start_time,
                animation_duration=1.5,  # Slide in duration
                animation_end_time=start_time + duration,
                max_width_percent=0.4,
                opacity=0.9
            )

            infographic_elements.append(reveal_config)

        # Apply infographic animations
        working_video = template_video
        temp_files = []

        for i, config in enumerate(infographic_elements):
            print(f"ðŸ“ˆ Adding infographic {i+1}/{len(infographic_elements)}...")

            temp_output = f"temp_infographic_{i:02d}.mp4"
            result = self.mm.watermark_video(working_video, config, temp_output)

            working_video = result.output_path
            temp_files.append(temp_output)

        # Final processing with transitions
        final_result = self.mm.add_smooth_transitions(
            working_video,
            output_path=output_path,
            transition_style='crossfade'
        )

        # Cleanup
        for temp_file in temp_files:
            if Path(temp_file).exists():
                Path(temp_file).unlink()

        return final_result

    def _create_text_graphic(self, text, font_size, color, font_family):
        """Create text graphic for animation."""
        # This would use PIL or similar to create text graphics
        # For demo purposes, return a placeholder path
        return f"generated_text_{hash(text)}.png"

    def _create_infographic_element(self, title, value, chart_type, color_scheme):
        """Create infographic element."""
        # This would generate charts/graphs using matplotlib or similar
        return f"infographic_{hash(title + str(value))}.png"

    def _create_slide_in_animation(self, direction, duration):
        """Create custom slide-in animation data."""
        return {
            'type': 'slide_in',
            'direction': direction,
            'duration': duration,
            'easing': 'ease_out'
        }

# Usage
animation_producer = ComplexAnimationProducer()

# Kinetic typography example
text_sequence = [
    {
        'text': 'Welcome to Our Company',
        'start_time': 2.0,
        'duration': 4.0,
        'animation': 'slide_in_left',
        'position': WatermarkPosition.CENTER,
        'scale': 0.5
    },
    {
        'text': 'Innovation Starts Here',
        'start_time': 7.0,
        'duration': 4.0,
        'animation': WatermarkAnimation.FADE_IN_OUT,
        'position': WatermarkPosition.CENTER,
        'scale': 0.4
    }
]

typography_result = animation_producer.create_kinetic_typography(
    text_sequence,
    "background_video.mp4",
    "kinetic_typography_result.mp4"
)

# Multi-logo sequence example
logo_sequence = [
    {
        'logo_path': 'partner_1.png',
        'start_time': 5.0,
        'position': WatermarkPosition.CENTER,
        'scale': 0.3
    },
    {
        'logo_path': 'partner_2.png',
        'start_time': 13.0,
        'position': WatermarkPosition.CENTER,
        'scale': 0.3
    }
]

multi_logo_result = animation_producer.create_multi_logo_sequence(
    "corporate_video.mp4",
    logo_sequence,
    "multi_logo_result.mp4"
)
```

### Advanced Timestamp and Frame Control

Sophisticated timestamp manipulation and frame-accurate editing:

```python
from xlibrary.media import MediaManager
from xlibrary.media.utils import TimeStampParser, parse_timestamp, frame_to_timestamp
import json

class PrecisionTimestampController:
    def __init__(self):
        self.mm = MediaManager()
        self.parser = TimeStampParser()

    def create_precise_cuts_from_script(self, video_path, script_file, output_dir):
        """Create precise video cuts based on detailed script timing."""

        # Load script with precise timestamps
        with open(script_file, 'r') as f:
            script_data = json.load(f)

        video_analysis = self.mm.analyze_video(video_path)
        video_fps = video_analysis.fps

        print(f"ðŸŽ¬ Processing script with {len(script_data['cuts'])} cuts")
        print(f"ðŸ“Š Video FPS: {video_fps}, Duration: {video_analysis.duration:.2f}s")

        cuts_created = []

        for i, cut in enumerate(script_data['cuts']):
            print(f"âœ‚ï¸ Processing cut {i+1}: {cut['name']}")

            # Parse start/end timestamps with high precision
            start_timestamp = self.parser.parse(cut['start'], video_fps)
            end_timestamp = self.parser.parse(cut['end'], video_fps)

            # Validate timestamps
            if start_timestamp.frame >= end_timestamp.frame:
                print(f"âš ï¸ Warning: Invalid cut range for {cut['name']}")
                continue

            if end_timestamp.seconds > video_analysis.duration:
                print(f"âš ï¸ Warning: End time exceeds video duration for {cut['name']}")
                end_timestamp = self.parser.parse(f"{video_analysis.duration:.3f}", video_fps)

            # Apply frame-accurate cuts
            cut_result = self.mm.trim_video(
                video_path,
                start_time=f"f{start_timestamp.frame}",  # Use exact frame numbers
                end_time=f"f{end_timestamp.frame}",
                output_path=Path(output_dir) / f"{cut['name']}.mp4",
                preserve_quality=True
            )

            # Calculate precise timing information
            actual_duration = (end_timestamp.frame - start_timestamp.frame) / video_fps

            cuts_created.append({
                'name': cut['name'],
                'output_path': cut_result.output_path,
                'start_frame': start_timestamp.frame,
                'end_frame': end_timestamp.frame,
                'start_seconds': start_timestamp.seconds,
                'end_seconds': end_timestamp.seconds,
                'duration_frames': end_timestamp.frame - start_timestamp.frame,
                'duration_seconds': actual_duration,
                'success': cut_result.success
            })

            print(f"   âœ… Created: {actual_duration:.3f}s ({end_timestamp.frame - start_timestamp.frame} frames)")

        return cuts_created

    def sync_audio_video_precise(self, video_path, audio_path, sync_offset, output_path):
        """Synchronize audio and video with frame-accurate precision."""

        print("ðŸ”„ Performing frame-accurate audio/video sync...")

        # Analyze both inputs
        video_analysis = self.mm.analyze_video(video_path)
        audio_analysis = self.mm.analyze_audio(audio_path)

        # Parse sync offset (can be positive or negative)
        offset_timestamp = self.parser.parse(sync_offset, video_analysis.fps)
        offset_frames = offset_timestamp.frame
        offset_seconds = offset_timestamp.seconds

        print(f"ðŸ“Š Sync offset: {offset_seconds:.3f}s ({offset_frames} frames)")

        if offset_seconds > 0:
            # Audio needs to be delayed
            print("â° Delaying audio...")
            sync_result = self.mm.sync_audio_video(
                video_path=video_path,
                audio_path=audio_path,
                audio_delay=offset_seconds,
                output_path=output_path
            )
        else:
            # Video needs to be delayed (crop audio start)
            print("â° Advancing audio...")
            sync_result = self.mm.sync_audio_video(
                video_path=video_path,
                audio_path=audio_path,
                audio_advance=abs(offset_seconds),
                output_path=output_path
            )

        return sync_result

    def create_frame_analysis_report(self, video_path, analysis_points):
        """Create detailed frame-by-frame analysis at specific points."""

        video_analysis = self.mm.analyze_video(video_path)
        fps = video_analysis.fps

        analysis_results = []

        print(f"ðŸ” Analyzing {len(analysis_points)} points in video...")

        for point in analysis_points:
            timestamp = self.parser.parse(point['time'], fps)

            # Extract frame at exact timestamp
            frame_path = f"analysis_frame_{timestamp.frame}.png"
            frame_result = self.mm.extract_frame(
                video_path,
                timestamp=f"f{timestamp.frame}",
                output_path=frame_path,
                high_quality=True
            )

            # Analyze frame properties
            frame_analysis = self.mm.analyze_frame(frame_path)

            analysis_data = {
                'analysis_point': point['name'],
                'timestamp_input': point['time'],
                'exact_frame': timestamp.frame,
                'exact_seconds': timestamp.seconds,
                'smpte_timecode': self.parser.convert_timestamp(timestamp, 'smpte'),
                'frame_path': frame_path,
                'frame_analysis': {
                    'average_brightness': frame_analysis.get('brightness', 0),
                    'dominant_colors': frame_analysis.get('colors', []),
                    'motion_vectors': frame_analysis.get('motion', {}),
                    'scene_change_score': frame_analysis.get('scene_change', 0)
                }
            }

            analysis_results.append(analysis_data)

            print(f"   ðŸ“Š {point['name']}: Frame {timestamp.frame} ({timestamp.seconds:.3f}s)")

        # Save analysis report
        report_path = f"frame_analysis_report_{Path(video_path).stem}.json"
        with open(report_path, 'w') as f:
            json.dump(analysis_results, f, indent=2)

        print(f"ðŸ“„ Analysis report saved: {report_path}")
        return analysis_results

    def create_precision_montage(self, source_clips, montage_script, output_path):
        """Create precision montage with exact frame timing."""

        print("ðŸŽžï¸ Creating precision montage...")

        montage_segments = []

        for i, segment in enumerate(montage_script['segments']):
            source_video = segment['source']

            # Parse precise timestamps
            start_ts = self.parser.parse(segment['start'], 30)  # Assume 30fps for parsing
            end_ts = self.parser.parse(segment['end'], 30)

            # Extract segment with exact timing
            segment_path = f"montage_segment_{i:03d}.mp4"
            segment_result = self.mm.trim_video(
                source_clips[source_video],
                start_time=f"f{start_ts.frame}",
                end_time=f"f{end_ts.frame}",
                output_path=segment_path,
                preserve_quality=True
            )

            # Apply segment-specific speed adjustment if specified
            if 'speed' in segment:
                speed_adjusted = self.mm.adjust_speed(
                    segment_path,
                    speed_factor=segment['speed'],
                    output_path=f"speed_adjusted_{i:03d}.mp4"
                )
                montage_segments.append(speed_adjusted.output_path)
                Path(segment_path).unlink()  # Clean up original
            else:
                montage_segments.append(segment_path)

        # Concatenate all segments
        montage_result = self.mm.concatenate_videos(
            montage_segments,
            output_path=output_path,
            add_transitions=montage_script.get('transitions', False),
            transition_duration=montage_script.get('transition_duration', 0.5)
        )

        # Cleanup segment files
        for segment_file in montage_segments:
            if Path(segment_file).exists():
                Path(segment_file).unlink()

        return montage_result

# Usage examples
controller = PrecisionTimestampController()

# Script-based precise cutting
script_cuts = {
    "cuts": [
        {"name": "intro", "start": "0:00", "end": "0:15.5"},
        {"name": "main_content", "start": "0:15.5", "end": "f7200"},
        {"name": "outro", "start": "f7200", "end": "4:30.75"}
    ]
}

with open("cut_script.json", "w") as f:
    json.dump(script_cuts, f, indent=2)

cuts_result = controller.create_precise_cuts_from_script(
    "master_video.mp4",
    "cut_script.json",
    "precise_cuts/"
)

# Frame analysis at specific points
analysis_points = [
    {"name": "opening_shot", "time": "0:05"},
    {"name": "mid_point", "time": "f3600"},
    {"name": "climax", "time": "2:45.33"}
]

frame_analysis = controller.create_frame_analysis_report(
    "master_video.mp4",
    analysis_points
)

# Precision montage
montage_script = {
    "segments": [
        {"source": 0, "start": "0:10", "end": "0:25", "speed": 1.2},
        {"source": 1, "start": "f450", "end": "f900"},
        {"source": 0, "start": "1:30.5", "end": "1:45"}
    ],
    "transitions": True,
    "transition_duration": 0.3
}

source_videos = ["clip_a.mp4", "clip_b.mp4"]

montage_result = controller.create_precision_montage(
    source_videos,
    montage_script,
    "precision_montage.mp4"
)
```

### Enterprise Batch Processing

Large-scale media processing for enterprise workflows:

```python
from xlibrary.media import MediaManager, ProcessingConfig, WatermarkConfig
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import queue
import threading
import logging
from dataclasses import dataclass
from typing import List, Dict, Optional

@dataclass
class BatchJob:
    id: str
    input_path: str
    output_path: str
    operation: str
    config: Dict
    priority: int = 0
    status: str = "pending"
    progress: float = 0.0
    error: Optional[str] = None

class EnterpriseBatchProcessor:
    def __init__(self, max_workers=4, queue_size=100):
        self.mm = MediaManager()
        self.max_workers = max_workers
        self.job_queue = queue.PriorityQueue(maxsize=queue_size)
        self.active_jobs = {}
        self.completed_jobs = {}
        self.failed_jobs = {}

        self.processing = False
        self.worker_threads = []

        # Setup logging
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)

    def start_processing(self):
        """Start the batch processing system."""
        if self.processing:
            return

        self.processing = True

        # Start worker threads
        for i in range(self.max_workers):
            worker = threading.Thread(
                target=self._worker_thread,
                name=f"BatchWorker-{i+1}",
                daemon=True
            )
            worker.start()
            self.worker_threads.append(worker)

        self.logger.info(f"Started batch processing with {self.max_workers} workers")

    def stop_processing(self):
        """Stop the batch processing system."""
        self.processing = False

        # Add sentinel values to stop workers
        for _ in range(self.max_workers):
            self.job_queue.put((float('inf'), None))

        # Wait for workers to finish
        for worker in self.worker_threads:
            worker.join()

        self.worker_threads.clear()
        self.logger.info("Stopped batch processing")

    def add_watermark_job(self, input_path: str, watermark_config: WatermarkConfig,
                         output_path: str, priority: int = 0) -> str:
        """Add watermark job to processing queue."""

        job_id = f"watermark_{len(self.active_jobs) + len(self.completed_jobs)}_{hash(input_path)}"

        job = BatchJob(
            id=job_id,
            input_path=input_path,
            output_path=output_path,
            operation="watermark",
            config={"watermark_config": watermark_config},
            priority=priority
        )

        # Higher priority = lower number (for queue ordering)
        self.job_queue.put((-priority, job))
        self.logger.info(f"Added watermark job: {job_id}")

        return job_id

    def add_batch_watermark_jobs(self, video_list: List[str],
                                watermark_config: WatermarkConfig,
                                output_dir: str, priority: int = 0) -> List[str]:
        """Add multiple watermark jobs for batch processing."""

        job_ids = []

        for video_path in video_list:
            input_path = Path(video_path)
            output_path = Path(output_dir) / f"watermarked_{input_path.name}"

            job_id = self.add_watermark_job(
                str(input_path),
                watermark_config,
                str(output_path),
                priority
            )

            job_ids.append(job_id)

        return job_ids

    def add_conversion_job(self, input_path: str, target_format: str,
                          output_path: str, quality_config: Dict,
                          priority: int = 0) -> str:
        """Add format conversion job to processing queue."""

        job_id = f"convert_{len(self.active_jobs) + len(self.completed_jobs)}_{hash(input_path)}"

        job = BatchJob(
            id=job_id,
            input_path=input_path,
            output_path=output_path,
            operation="convert",
            config={
                "target_format": target_format,
                "quality_config": quality_config
            },
            priority=priority
        )

        self.job_queue.put((-priority, job))
        self.logger.info(f"Added conversion job: {job_id}")

        return job_id

    def add_trim_job(self, input_path: str, start_time: str, end_time: str,
                    output_path: str, priority: int = 0) -> str:
        """Add video trim job to processing queue."""

        job_id = f"trim_{len(self.active_jobs) + len(self.completed_jobs)}_{hash(input_path)}"

        job = BatchJob(
            id=job_id,
            input_path=input_path,
            output_path=output_path,
            operation="trim",
            config={
                "start_time": start_time,
                "end_time": end_time
            },
            priority=priority
        )

        self.job_queue.put((-priority, job))
        self.logger.info(f"Added trim job: {job_id}")

        return job_id

    def _worker_thread(self):
        """Worker thread for processing jobs."""

        while self.processing:
            try:
                # Get next job from queue
                priority, job = self.job_queue.get(timeout=1.0)

                if job is None:  # Sentinel value to stop worker
                    break

                # Process the job
                self._process_job(job)

                self.job_queue.task_done()

            except queue.Empty:
                continue
            except Exception as e:
                self.logger.error(f"Worker thread error: {e}")

    def _process_job(self, job: BatchJob):
        """Process a single job."""

        self.active_jobs[job.id] = job
        job.status = "processing"

        self.logger.info(f"Processing job {job.id}: {job.operation}")

        try:
            if job.operation == "watermark":
                result = self._process_watermark_job(job)
            elif job.operation == "convert":
                result = self._process_conversion_job(job)
            elif job.operation == "trim":
                result = self._process_trim_job(job)
            else:
                raise ValueError(f"Unknown operation: {job.operation}")

            if result and result.success:
                job.status = "completed"
                job.progress = 100.0
                self.completed_jobs[job.id] = job
                self.logger.info(f"Completed job {job.id}")
            else:
                job.status = "failed"
                job.error = result.error if result else "Unknown error"
                self.failed_jobs[job.id] = job
                self.logger.error(f"Failed job {job.id}: {job.error}")

        except Exception as e:
            job.status = "failed"
            job.error = str(e)
            self.failed_jobs[job.id] = job
            self.logger.error(f"Failed job {job.id}: {e}")

        finally:
            # Remove from active jobs
            self.active_jobs.pop(job.id, None)

    def _process_watermark_job(self, job: BatchJob):
        """Process watermark job."""

        watermark_config = job.config["watermark_config"]

        # Add progress callback
        def progress_callback(current, total, message):
            job.progress = (current / total) * 100
            self.logger.debug(f"Job {job.id} progress: {job.progress:.1f}%")

        return self.mm.watermark_video(
            job.input_path,
            watermark_config,
            job.output_path,
            progress_callback=progress_callback
        )

    def _process_conversion_job(self, job: BatchJob):
        """Process format conversion job."""

        target_format = job.config["target_format"]
        quality_config = job.config["quality_config"]

        return self.mm.convert_video_format(
            job.input_path,
            target_format=target_format,
            output_path=job.output_path,
            **quality_config
        )

    def _process_trim_job(self, job: BatchJob):
        """Process video trim job."""

        start_time = job.config["start_time"]
        end_time = job.config["end_time"]

        return self.mm.trim_video(
            job.input_path,
            start_time=start_time,
            end_time=end_time,
            output_path=job.output_path
        )

    def get_job_status(self, job_id: str) -> Optional[BatchJob]:
        """Get status of specific job."""

        # Check active jobs
        if job_id in self.active_jobs:
            return self.active_jobs[job_id]

        # Check completed jobs
        if job_id in self.completed_jobs:
            return self.completed_jobs[job_id]

        # Check failed jobs
        if job_id in self.failed_jobs:
            return self.failed_jobs[job_id]

        return None

    def get_queue_status(self) -> Dict:
        """Get overall queue status."""

        return {
            "queued": self.job_queue.qsize(),
            "active": len(self.active_jobs),
            "completed": len(self.completed_jobs),
            "failed": len(self.failed_jobs),
            "processing": self.processing
        }

    def wait_for_completion(self, timeout: Optional[float] = None):
        """Wait for all jobs to complete."""

        self.job_queue.join()

        # Wait for active jobs to finish
        import time
        start_time = time.time()

        while self.active_jobs:
            if timeout and (time.time() - start_time) > timeout:
                break
            time.sleep(0.1)

    def generate_report(self) -> Dict:
        """Generate processing report."""

        total_jobs = len(self.completed_jobs) + len(self.failed_jobs)
        success_rate = len(self.completed_jobs) / total_jobs if total_jobs > 0 else 0

        report = {
            "summary": {
                "total_jobs": total_jobs,
                "completed": len(self.completed_jobs),
                "failed": len(self.failed_jobs),
                "success_rate": success_rate
            },
            "completed_jobs": list(self.completed_jobs.keys()),
            "failed_jobs": [
                {"id": job.id, "error": job.error}
                for job in self.failed_jobs.values()
            ]
        }

        return report

# Usage example
processor = EnterpriseBatchProcessor(max_workers=6)
processor.start_processing()

# Add batch watermark jobs
video_list = [
    "content_01.mp4", "content_02.mp4", "content_03.mp4",
    "content_04.mp4", "content_05.mp4"
]

watermark_config = WatermarkConfig(
    watermark_path="company_logo.png",
    position=WatermarkPosition.BOTTOM_RIGHT,
    auto_scale=True,
    opacity=0.8
)

watermark_jobs = processor.add_batch_watermark_jobs(
    video_list,
    watermark_config,
    "branded_videos/",
    priority=1
)

# Add conversion jobs
for video in video_list:
    processor.add_conversion_job(
        video,
        target_format="webm",
        output_path=f"web_versions/{Path(video).stem}.webm",
        quality_config={"crf": 30, "codec": "vp9"},
        priority=2
    )

# Monitor progress
import time

while processor.get_queue_status()["queued"] > 0 or processor.get_queue_status()["active"] > 0:
    status = processor.get_queue_status()
    print(f"Queue: {status['queued']}, Active: {status['active']}, "
          f"Completed: {status['completed']}, Failed: {status['failed']}")
    time.sleep(5)

# Generate final report
final_report = processor.generate_report()
print(f"Processing complete! Success rate: {final_report['summary']['success_rate']:.1%}")

processor.stop_processing()
```

This comprehensive guide demonstrates the full power and flexibility of xlibrary's Media Manager. From professional video editing to enterprise batch processing, it provides sophisticated tools for any media processing workflow with precision, quality, and efficiency.

---

## Next Steps

- **[Chapter 6: Communication Manager](06.01%20Chapter%206%20-%20Communication%20Manager%20-%20Design%20-%20Overview.md)** - Email and messaging systems
- **[Design Documentation](05.01%20Chapter%205%20-%20Media%20Manager%20-%20Design%20-%20Overview.md)** - Architecture deep dive
- **[xlibrary Core Concepts](00.00%20Chapter%200%20-%20Introduction.md)** - Understanding the xlibrary ecosystem

**Master professional media processing with xlibrary's comprehensive Media Manager!** ðŸŽ¬