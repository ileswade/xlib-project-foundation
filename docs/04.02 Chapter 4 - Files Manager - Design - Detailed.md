# Chapter 4: Files Manager - Design Detailed

> **üîç DEEP DIVE INTO FILE OPERATIONS ARCHITECTURE**
> Comprehensive exploration of xlibrary's Files Manager internals: deduplication algorithms, type detection systems, compression strategies, and organizational workflows.

## Architecture Deep Dive

### Core Architecture Overview

The Files Manager employs a modular architecture with specialized components handling different aspects of file operations:

```python
FileManager
‚îú‚îÄ‚îÄ FileOperations      # Basic file operations and batch processing
‚îú‚îÄ‚îÄ DuplicateFinder    # Advanced duplicate detection algorithms
‚îú‚îÄ‚îÄ CompressionManager # Multi-format archive operations
‚îî‚îÄ‚îÄ TypeDetector       # File type analysis and validation

# Component initialization
fm = FileManager()
fm.operations       # FileOperations instance
fm.deduplicator    # DuplicateFinder instance
fm.compressor      # CompressionManager instance
```

### Type Detection System Architecture

#### Magic Number Detection Implementation

The type detection system uses multiple layers of analysis:

```python
class TypeDetector:
    def __init__(self):
        # Primary: libmagic for content analysis
        self.magic_instance = magic.Magic(mime=True)
        self.magic_type_instance = magic.Magic()

        # Fallback: Python mimetypes
        self.mimetypes_instance = mimetypes.MimeTypes()

        # Custom rules for special cases
        self.custom_patterns = {
            b'\x89PNG\r\n\x1a\n': 'image/png',
            b'\xff\xd8\xff': 'image/jpeg',
            b'PK\x03\x04': 'application/zip'
        }

    def detect_type(self, file_path):
        """Multi-layer type detection."""
        # Layer 1: Magic number analysis
        try:
            content_type = self.magic_instance.from_file(str(file_path))
            description = self.magic_type_instance.from_file(str(file_path))
        except:
            content_type, description = None, None

        # Layer 2: Extension-based detection
        extension_type, _ = self.mimetypes_instance.guess_type(str(file_path))

        # Layer 3: Custom pattern matching
        custom_type = self._check_custom_patterns(file_path)

        # Conflict resolution
        return self._resolve_type_conflicts(
            content_type, extension_type, custom_type, file_path
        )
```

#### Type Classification System

```python
class FileTypeClassifier:
    """Advanced file type classification beyond MIME types."""

    TYPE_MAPPINGS = {
        # Images
        'image/jpeg': FileType.IMAGE,
        'image/png': FileType.IMAGE,
        'image/gif': FileType.IMAGE,
        'image/webp': FileType.IMAGE,
        'image/svg+xml': FileType.IMAGE,

        # Documents
        'application/pdf': FileType.DOCUMENT,
        'application/msword': FileType.DOCUMENT,
        'application/vnd.openxmlformats-officedocument.wordprocessingml.document': FileType.DOCUMENT,
        'text/plain': FileType.TEXT,
        'text/html': FileType.TEXT,

        # Archives
        'application/zip': FileType.ARCHIVE,
        'application/x-tar': FileType.ARCHIVE,
        'application/gzip': FileType.ARCHIVE,

        # Executables
        'application/x-executable': FileType.EXECUTABLE,
        'application/x-msdos-program': FileType.EXECUTABLE
    }

    def classify(self, mime_type: str, file_path: Path) -> FileType:
        """Classify file type with context-aware rules."""
        # Direct mapping
        if mime_type in self.TYPE_MAPPINGS:
            return self.TYPE_MAPPINGS[mime_type]

        # Pattern-based classification
        if mime_type.startswith('image/'):
            return FileType.IMAGE
        elif mime_type.startswith('video/'):
            return FileType.VIDEO
        elif mime_type.startswith('audio/'):
            return FileType.AUDIO
        elif mime_type.startswith('text/'):
            return FileType.TEXT

        # Extension-based fallback
        return self._classify_by_extension(file_path.suffix.lower())
```

### Deduplication Algorithm Architecture

#### Multi-Strategy Duplicate Detection

```python
class DuplicateFinder:
    """Advanced duplicate detection with multiple strategies."""

    def __init__(self, file_manager):
        self.file_manager = file_manager
        self.hash_cache = {}  # Performance optimization

    def find_duplicates(self, directory: Path, strategies: List[str]) -> List[DuplicateGroup]:
        """
        Find duplicates using multiple strategies.

        Strategies:
        - 'size': Group by file size (fast pre-filter)
        - 'hash': Group by content hash (accurate)
        - 'fuzzy': Fuzzy content matching (similar files)
        - 'metadata': Name/date similarity
        """
        candidates = []

        if 'size' in strategies:
            candidates = self._find_size_duplicates(directory)

        if 'hash' in strategies:
            candidates = self._refine_with_hash(candidates)

        if 'fuzzy' in strategies:
            candidates = self._add_fuzzy_matches(candidates, directory)

        if 'metadata' in strategies:
            candidates = self._add_metadata_matches(candidates, directory)

        return self._group_duplicates(candidates)
```

#### Hash-Based Deduplication Implementation

```python
def _calculate_file_hash(self, file_path: Path, algorithm: str = 'md5') -> str:
    """Efficient hash calculation with caching."""
    cache_key = f"{file_path}:{file_path.stat().st_mtime}:{algorithm}"

    if cache_key in self.hash_cache:
        return self.hash_cache[cache_key]

    hasher = hashlib.new(algorithm)

    try:
        with open(file_path, 'rb') as f:
            # Process in chunks for memory efficiency
            while chunk := f.read(self.file_manager.chunk_size):
                hasher.update(chunk)

        hash_value = hasher.hexdigest()
        self.hash_cache[cache_key] = hash_value
        return hash_value

    except Exception as e:
        raise FileManagerError(f"Hash calculation failed for {file_path}: {e}")

def _find_hash_duplicates(self, files: List[Path]) -> Dict[str, List[Path]]:
    """Group files by content hash."""
    hash_groups = {}

    for file_path in files:
        try:
            file_hash = self._calculate_file_hash(file_path)
            if file_hash not in hash_groups:
                hash_groups[file_hash] = []
            hash_groups[file_hash].append(file_path)
        except FileManagerError:
            continue  # Skip files that cannot be hashed

    # Return only groups with multiple files
    return {k: v for k, v in hash_groups.items() if len(v) > 1}
```

#### Fuzzy Matching Algorithm

```python
def _fuzzy_content_match(self, file1: Path, file2: Path, threshold: float = 0.9) -> bool:
    """Compare files using fuzzy matching for similar content."""
    try:
        # Sample-based comparison for large files
        sample_size = min(8192, file1.stat().st_size, file2.stat().st_size)

        with open(file1, 'rb') as f1, open(file2, 'rb') as f2:
            # Compare multiple samples throughout the file
            samples = 5
            for i in range(samples):
                offset = (sample_size * i) // samples

                f1.seek(offset)
                f2.seek(offset)

                chunk1 = f1.read(sample_size // samples)
                chunk2 = f2.read(sample_size // samples)

                similarity = self._calculate_similarity(chunk1, chunk2)
                if similarity < threshold:
                    return False

        return True

    except Exception:
        return False

def _calculate_similarity(self, data1: bytes, data2: bytes) -> float:
    """Calculate similarity between two byte sequences."""
    if len(data1) != len(data2):
        return 0.0

    matching_bytes = sum(b1 == b2 for b1, b2 in zip(data1, data2))
    return matching_bytes / len(data1) if data1 else 0.0
```

### Compression System Architecture

#### Multi-Format Compression Manager

```python
class CompressionManager:
    """Handles multiple compression formats with unified interface."""

    SUPPORTED_FORMATS = {
        'zip': ZipCompressor,
        'tar': TarCompressor,
        'tar.gz': TarGzCompressor,
        'tar.bz2': TarBz2Compressor,
        'tar.xz': TarXzCompressor
    }

    def __init__(self, file_manager):
        self.file_manager = file_manager
        self.compressors = {}

    def compress_folder(self, source: Path, destination: Path,
                       format_type: str, **kwargs) -> CompressionResult:
        """Compress folder using specified format."""
        if format_type not in self.SUPPORTED_FORMATS:
            raise FileManagerError(f"Unsupported compression format: {format_type}")

        compressor = self._get_compressor(format_type)
        return compressor.compress(source, destination, **kwargs)

    def _get_compressor(self, format_type: str):
        """Get or create compressor instance."""
        if format_type not in self.compressors:
            compressor_class = self.SUPPORTED_FORMATS[format_type]
            self.compressors[format_type] = compressor_class(self.file_manager)
        return self.compressors[format_type]
```

#### ZIP Compression Implementation

```python
class ZipCompressor:
    """ZIP format compression with progress tracking."""

    def __init__(self, file_manager):
        self.file_manager = file_manager

    def compress(self, source: Path, destination: Path,
                compression_level: int = 6,
                progress_callback: Optional[ProgressCallback] = None) -> CompressionResult:
        """Compress folder to ZIP with progress tracking."""

        # Collect all files first for progress calculation
        all_files = list(source.rglob('*'))
        file_list = [f for f in all_files if f.is_file()]
        total_files = len(file_list)
        total_size = sum(f.stat().st_size for f in file_list)

        processed_files = 0
        processed_size = 0

        try:
            with zipfile.ZipFile(destination, 'w', zipfile.ZIP_DEFLATED,
                               compresslevel=compression_level) as zipf:

                for file_path in file_list:
                    # Calculate relative path for archive
                    arcname = file_path.relative_to(source)

                    # Add file to archive
                    zipf.write(file_path, arcname)

                    # Update progress
                    processed_files += 1
                    processed_size += file_path.stat().st_size

                    if progress_callback:
                        progress_callback(
                            processed_files,
                            total_files,
                            f"Compressing {file_path.name}"
                        )

            return CompressionResult(
                success=True,
                source_path=source,
                destination_path=destination,
                files_compressed=total_files,
                original_size=total_size,
                compressed_size=destination.stat().st_size,
                compression_ratio=(destination.stat().st_size / total_size) if total_size > 0 else 0
            )

        except Exception as e:
            return CompressionResult(
                success=False,
                error=str(e)
            )
```

### Folder Operations Architecture

#### Intelligent Folder Collapse

```python
class FolderOperations:
    """Advanced folder structure manipulation."""

    def collapse_folders(self, root_path: Path,
                        preserve_structure: bool = False,
                        dry_run: bool = False) -> FolderOperationResult:
        """
        Collapse nested folder structure.

        Args:
            preserve_structure: Keep folder names as prefixes
            dry_run: Test mode without actual file operations
        """

        # Analysis phase
        all_files = []
        folder_structure = {}

        for file_path in root_path.rglob('*'):
            if file_path.is_file():
                relative_path = file_path.relative_to(root_path)
                folder_path = relative_path.parent

                all_files.append({
                    'source': file_path,
                    'relative': relative_path,
                    'folder': folder_path,
                    'new_name': self._generate_collapsed_name(relative_path, preserve_structure)
                })

        # Conflict detection
        conflicts = self._detect_name_conflicts(all_files)
        if conflicts:
            resolved_files = self._resolve_conflicts(all_files, conflicts)
        else:
            resolved_files = all_files

        if dry_run:
            return FolderOperationResult(
                success=True,
                files_processed=len(resolved_files),
                dry_run=True,
                planned_operations=resolved_files
            )

        # Execution phase
        return self._execute_collapse(root_path, resolved_files)

    def _generate_collapsed_name(self, relative_path: Path, preserve_structure: bool) -> str:
        """Generate new filename for collapsed structure."""
        if not preserve_structure or len(relative_path.parts) == 1:
            return relative_path.name

        # Create filename with folder prefixes
        folder_parts = relative_path.parts[:-1]
        filename = relative_path.name

        # Clean folder names for filename use
        clean_parts = [self._sanitize_for_filename(part) for part in folder_parts]
        prefix = '_'.join(clean_parts)

        name_part, ext_part = filename.rsplit('.', 1) if '.' in filename else (filename, '')

        if ext_part:
            return f"{prefix}_{name_part}.{ext_part}"
        else:
            return f"{prefix}_{name_part}"
```

#### File Organization by Type

```python
class FileOrganizer:
    """Intelligent file organization strategies."""

    TYPE_FOLDER_MAPPING = {
        FileType.IMAGE: "Images",
        FileType.VIDEO: "Videos",
        FileType.AUDIO: "Audio",
        FileType.DOCUMENT: "Documents",
        FileType.TEXT: "Text Files",
        FileType.ARCHIVE: "Archives",
        FileType.EXECUTABLE: "Programs",
        FileType.BINARY: "Binary Files",
        FileType.UNKNOWN: "Other"
    }

    def organize_by_type(self, source_path: Path,
                        destination_path: Optional[Path] = None,
                        custom_mapping: Optional[Dict[FileType, str]] = None) -> OrganizationResult:
        """Organize files by detected type."""

        dest_path = destination_path or source_path
        type_mapping = custom_mapping or self.TYPE_FOLDER_MAPPING

        # Analyze all files
        file_analysis = {}
        folder_stats = {}

        for file_path in source_path.rglob('*'):
            if file_path.is_file():
                file_info = self.file_manager.detect_file_type(file_path)
                file_analysis[file_path] = file_info

                target_folder = type_mapping.get(file_info.file_type, "Other")
                if target_folder not in folder_stats:
                    folder_stats[target_folder] = {'count': 0, 'size': 0}

                folder_stats[target_folder]['count'] += 1
                folder_stats[target_folder]['size'] += file_info.size

        # Create destination folders
        created_folders = []
        for folder_name in folder_stats.keys():
            folder_path = dest_path / folder_name
            if not folder_path.exists():
                folder_path.mkdir(parents=True, exist_ok=True)
                created_folders.append(folder_path)

        # Move files to appropriate folders
        moved_files = 0
        total_size_moved = 0
        errors = []

        for file_path, file_info in file_analysis.items():
            try:
                target_folder = type_mapping.get(file_info.file_type, "Other")
                target_path = dest_path / target_folder / file_path.name

                # Handle name conflicts
                if target_path.exists():
                    target_path = self._resolve_naming_conflict(target_path)

                shutil.move(str(file_path), str(target_path))
                moved_files += 1
                total_size_moved += file_info.size

            except Exception as e:
                errors.append(f"Failed to move {file_path}: {str(e)}")

        return OrganizationResult(
            success=len(errors) == 0,
            files_processed=moved_files,
            folders_created=created_folders,
            total_size_organized=total_size_moved,
            folder_statistics=folder_stats,
            errors=errors
        )
```

### Batch Processing Architecture

#### Efficient Batch Operations

```python
class BatchProcessor:
    """Handles large-scale file operations efficiently."""

    def __init__(self, file_manager):
        self.file_manager = file_manager
        self.batch_size = 100
        self.max_workers = 4

    def process_files_in_batches(self, file_list: List[Path],
                                operation: Callable,
                                progress_callback: Optional[ProgressCallback] = None) -> BatchResult:
        """Process files in manageable batches."""

        total_files = len(file_list)
        processed_files = 0
        successful_operations = 0
        failed_operations = []

        # Process in batches
        for batch_start in range(0, total_files, self.batch_size):
            batch_end = min(batch_start + self.batch_size, total_files)
            batch = file_list[batch_start:batch_end]

            batch_results = self._process_batch(batch, operation)

            # Update counters
            processed_files += len(batch)
            successful_operations += batch_results['successful']
            failed_operations.extend(batch_results['failed'])

            # Progress callback
            if progress_callback:
                progress_callback(
                    processed_files,
                    total_files,
                    f"Processed batch {batch_start//self.batch_size + 1}"
                )

            # Memory management - clear batch results
            del batch_results

        return BatchResult(
            total_files=total_files,
            successful_operations=successful_operations,
            failed_operations=failed_operations,
            success_rate=(successful_operations / total_files) if total_files > 0 else 0
        )

    def _process_batch(self, batch: List[Path], operation: Callable) -> Dict:
        """Process a single batch of files."""
        successful = 0
        failed = []

        for file_path in batch:
            try:
                result = operation(file_path)
                if result.success:
                    successful += 1
                else:
                    failed.append({'file': file_path, 'error': result.error})
            except Exception as e:
                failed.append({'file': file_path, 'error': str(e)})

        return {'successful': successful, 'failed': failed}
```

### Error Handling and Recovery

#### Comprehensive Error Management

```python
class FileManagerError(Exception):
    """Base exception for file manager operations."""

    def __init__(self, message: str, operation_id: Optional[str] = None,
                 rollback_data: Optional[Dict] = None):
        super().__init__(message)
        self.operation_id = operation_id
        self.rollback_data = rollback_data
        self.rollback_available = rollback_data is not None

class OperationRecovery:
    """Handles operation rollback and recovery."""

    def __init__(self):
        self.operation_log = {}

    def log_operation(self, operation_id: str, operation_type: str,
                     files_affected: List[Path], rollback_data: Dict):
        """Log operation for potential rollback."""
        self.operation_log[operation_id] = {
            'type': operation_type,
            'timestamp': datetime.now(),
            'files': files_affected,
            'rollback_data': rollback_data,
            'status': 'in_progress'
        }

    def rollback_operation(self, operation_id: str) -> bool:
        """Rollback a logged operation."""
        if operation_id not in self.operation_log:
            return False

        operation = self.operation_log[operation_id]
        rollback_data = operation['rollback_data']

        try:
            if operation['type'] == 'move_files':
                return self._rollback_move_files(rollback_data)
            elif operation['type'] == 'organize_files':
                return self._rollback_organize_files(rollback_data)
            elif operation['type'] == 'compress_folder':
                return self._rollback_compress_folder(rollback_data)

        except Exception as e:
            logger.error(f"Rollback failed for {operation_id}: {e}")
            return False

        return True
```

### Performance Optimization Strategies

#### Memory-Efficient Processing

```python
class MemoryOptimizedProcessor:
    """Optimized processing for large file sets."""

    def __init__(self, file_manager):
        self.file_manager = file_manager
        self.memory_threshold = 512 * 1024 * 1024  # 512MB

    def process_large_directory(self, directory: Path,
                               operation: str) -> ProcessingResult:
        """Process large directories with memory optimization."""

        # Estimate memory usage
        estimated_files = self._estimate_file_count(directory)
        estimated_memory = estimated_files * 1024  # Rough estimate per file

        if estimated_memory > self.memory_threshold:
            return self._process_with_streaming(directory, operation)
        else:
            return self._process_normal(directory, operation)

    def _process_with_streaming(self, directory: Path, operation: str):
        """Stream-based processing for large directories."""
        def file_generator():
            for file_path in directory.rglob('*'):
                if file_path.is_file():
                    yield file_path

        processed = 0
        errors = []

        for file_path in file_generator():
            try:
                result = self._execute_operation(file_path, operation)
                if result.success:
                    processed += 1
                else:
                    errors.append(result.error)

                # Periodic memory cleanup
                if processed % 100 == 0:
                    self._cleanup_memory()

            except Exception as e:
                errors.append(str(e))

        return ProcessingResult(
            files_processed=processed,
            errors=errors,
            memory_optimized=True
        )
```

This detailed architecture provides the foundation for xlibrary's comprehensive file management system, enabling sophisticated operations while maintaining performance and reliability.

---

## Next Steps

- **[User Guide Overview](04.03%20Chapter%204%20-%20Files%20Manager%20-%20User%20Guide%20-%20Overview.md)** - Quick start guide for file operations
- **[User Guide Detailed](04.04%20Chapter%204%20-%20Files%20Manager%20-%20User%20Guide%20-%20Detailed.md)** - Comprehensive usage patterns and examples
- **[Chapter 5: Media Manager](05.01%20Chapter%205%20-%20Media%20Manager%20-%20Design%20-%20Overview.md)** - Media processing and manipulation

**Deep understanding of Files Manager architecture enables powerful and efficient file operations.** üîç