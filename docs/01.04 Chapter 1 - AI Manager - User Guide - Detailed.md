# Chapter 1: AI Manager - Detailed User Guide

> **ðŸ“š COMPREHENSIVE FEATURE REFERENCE**
> Complete coverage of all AI Manager features, advanced patterns, and production configurations. Your definitive guide to mastering the AI pillar.

## Table of Contents

- [Advanced Configuration](#advanced-configuration)
- [Provider-Specific Features](#provider-specific-features)
- [Conversation Management](#conversation-management)
- [Streaming and Real-Time Features](#streaming-and-real-time-features)
- [File Attachments and Vision](#file-attachments-and-vision)
- [Production Features](#production-features)
- [Testing and Debugging](#testing-and-debugging)
- [Performance Optimization](#performance-optimization)
- [Integration Patterns](#integration-patterns)

---

## Advanced Configuration

### Complete AIConfig Reference

```python
from xlibrary.ai import AIManager, AIConfig

config = AIConfig(
    # Core provider settings
    provider="claude",                    # Provider: claude, openai, deepseek, mock
    api_key="your-api-key",              # API key (required)
    base_url=None,                       # Custom API endpoint (optional)
    model="current",                     # Model name or universal alias

    # Request parameters
    max_tokens=4096,                     # Maximum response tokens
    temperature=0.7,                     # Creativity level (0.0-2.0)
    top_p=None,                         # Nucleus sampling (provider dependent)
    frequency_penalty=None,              # Reduce repetition (OpenAI only)
    presence_penalty=None,               # Encourage topic diversity (OpenAI only)

    # Timeout and retry settings
    timeout=30.0,                       # Request timeout in seconds
    retries=3,                          # Number of retry attempts
    retry_delay=1.0,                    # Delay between retries
    exponential_backoff=True,           # Use exponential backoff for retries

    # Feature flags
    enable_streaming=True,              # Support streaming responses
    enable_reasoning=False,             # Enable reasoning mode where supported
    enable_vision=True,                 # Support vision/image inputs
    enable_function_calling=True,       # Support function calling (OpenAI)
    enable_structured_output=True,      # Support structured JSON output

    # Production features
    enable_metrics=False,               # Track usage metrics
    enable_health_checks=False,         # Monitor provider health
    enable_rate_limiting=False,         # Apply rate limiting
    enable_logging=False,               # Structured logging
    enable_caching=False,               # Response caching

    # Security settings
    content_filtering=True,             # Apply content safety filters
    api_key_validation=True,            # Validate API key format
    request_validation=True,            # Validate request parameters

    # Advanced settings
    user_agent="xlibrary/1.0.0",       # Custom user agent
    additional_headers=None,            # Extra HTTP headers
    proxy_url=None,                     # HTTP proxy configuration
    verify_ssl=True                     # SSL certificate verification
)

ai = AIManager(
    api_key="your-api-key",
    provider="claude",
    model="current",
    config=config
)
```

### Configuration from External Sources

The AI Manager follows an explicit configuration philosophy - API keys must always be provided directly, never from environment variables. This design ensures security clarity and prevents hidden dependencies. However, you can load other non-sensitive configuration from files or environment variables:

```python
import os
from dotenv import load_dotenv
from xlibrary.ai import AIManager

load_dotenv()  # Load .env file for other configuration

# Load API key from secure source - NEVER from environment variables
# This demonstrates the explicit approach - you control exactly where credentials come from
api_key = get_api_key_from_secure_storage()  # Your secure key retrieval method

# Use environment for non-sensitive configuration only
# The constructor requires explicit api_key, defaults provider="claude", model="latest"
ai = AIManager(
    api_key=api_key,  # Always explicit - this is the only required parameter
    provider=os.getenv("AI_DEFAULT_PROVIDER", "claude"),  # Optional, defaults to "claude"
    model=os.getenv("AI_DEFAULT_MODEL", "latest"),        # Optional, defaults to "latest"
    max_tokens=int(os.getenv("AI_MAX_TOKENS", "4096")),
    temperature=float(os.getenv("AI_TEMPERATURE", "0.7")),
    enable_metrics=os.getenv("AI_ENABLE_METRICS", "false").lower() == "true"
)
```

### Configuration Profiles

Create reusable configuration profiles:

```python
from xlibrary.ai import AIConfig

# Development profile
dev_config = AIConfig(
    provider="mock",                    # Use mock for development
    model="latest",
    enable_metrics=True,
    enable_logging=True,
    content_filtering=False            # Disable for testing
)

# Production profile
prod_config = AIConfig(
    provider="claude",
    model="current",
    max_tokens=2048,
    temperature=0.5,                   # More deterministic
    enable_metrics=True,
    enable_health_checks=True,
    enable_rate_limiting=True,
    enable_logging=True,
    content_filtering=True             # Enable safety filters
)

# High-performance profile
performance_config = AIConfig(
    provider="openai",
    model="fast",
    max_tokens=1024,
    temperature=0.3,
    timeout=15.0,                      # Faster timeout
    enable_caching=True                # Cache responses
)

# Use profiles - notice the explicit API key requirement in each case
# The api_key parameter is always first and required, other parameters have smart defaults
dev_ai = AIManager(api_key="test-key", provider="mock", config=dev_config)
prod_ai = AIManager(api_key="your-claude-key", provider="claude", config=prod_config)
fast_ai = AIManager(api_key="your-openai-key", provider="openai", config=performance_config)
```

---

## Provider-Specific Features

### Claude (Anthropic) Features

```python
from xlibrary.ai import AIManager

# Create AI manager with Claude provider (the default)
# Notice the constructor signature: api_key is required, provider defaults to "claude", model defaults to "latest"
ai = AIManager(api_key="your-claude-key", model="current")

# Reasoning mode - Claude thinks step by step before responding
# This uses the new method chaining API: request() returns a ChainableRequest that you can configure
response = ai.request(
    "Solve this logic puzzle: If all roses are flowers, and some flowers are red, can we conclude that some roses are red?"
).with_reasoning(True).with_max_tokens(4000).send()

print("Answer:", response.content)
if hasattr(response, 'reasoning'):
    print("Thinking process:", response.reasoning)

# Artifacts support - Claude can create structured content
response = ai.request(
    "Create a Python function to calculate fibonacci numbers",
    enable_artifacts=True
)

# Access artifacts if available
if response.artifacts:
    for artifact in response.artifacts:
        print(f"Artifact type: {artifact.type}")
        print(f"Content: {artifact.content}")

# Vision capabilities - analyze images
response = ai.request(
    "Describe what you see in this image and identify any text",
    attachments=["image.jpg"],
    max_tokens=1000
)

# Claude-specific parameters
response = ai.request(
    "Write a creative story",
    top_k=40,                          # Claude-specific parameter
    stop_sequences=["THE END"],        # Custom stop sequences
    claude_system_message="You are a creative writer"  # System message
)
```

### OpenAI Features

```python
from xlibrary.ai import AIManager

ai = AIManager(provider="openai", api_key="your-api-key", model="gpt-4o")

# Reasoning with o1 models
reasoning_ai = AIManager(provider="openai", api_key="your-api-key", model="reasoning")  # Uses o1-preview
response = reasoning_ai.request(
    "Prove that the square root of 2 is irrational",
    reasoning_mode=True
)

# Function calling
functions = [
    {
        "name": "get_weather",
        "description": "Get current weather for a location",
        "parameters": {
            "type": "object",
            "properties": {
                "location": {"type": "string", "description": "City name"},
                "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]}
            },
            "required": ["location"]
        }
    }
]

response = ai.request(
    "What's the weather like in Tokyo?",
    functions=functions,
    function_call="auto"  # Let model decide whether to call function
)

# Structured output with JSON schema
schema = {
    "type": "object",
    "properties": {
        "summary": {"type": "string"},
        "sentiment": {"type": "string", "enum": ["positive", "negative", "neutral"]},
        "topics": {"type": "array", "items": {"type": "string"}}
    },
    "required": ["summary", "sentiment", "topics"]
}

response = ai.request(
    "Analyze this product review: 'Great product, fast delivery, would buy again!'",
    response_format={"type": "json_object", "schema": schema}
)

# OpenAI-specific parameters
response = ai.request(
    "Write a technical explanation",
    frequency_penalty=0.5,             # Reduce repetition
    presence_penalty=0.3,              # Encourage new topics
    logit_bias={50256: -100},          # Bias against specific tokens
    user="user123"                     # User tracking
)
```

### DeepSeek Features

```python
from xlibrary.ai import AIManager

ai = AIManager(provider="deepseek", api_key="your-api-key", model="latest")

# DeepSeek reasoning with R1 model
reasoning_ai = AIManager(provider="deepseek", api_key="your-api-key", model="reasoning")  # Uses deepseek-reasoner
response = reasoning_ai.request(
    "Design an algorithm to solve the traveling salesman problem efficiently",
    reasoning_mode=True,
    max_tokens=8000
)

# Access thinking process
if hasattr(response, 'reasoning'):
    print("Reasoning steps:", response.reasoning)

# DeepSeek MoE (Mixture of Experts) optimization
response = ai.request(
    "Explain quantum computing",
    moe_routing="expert",              # Route to specific expert
    temperature=0.6
)

# Code-focused requests (DeepSeek excels at code)
response = ai.request(
    "Write a Python class for a binary search tree with insert, search, and delete methods",
    max_tokens=3000,
    stop_sequences=["```"],
    focus_mode="code"                  # DeepSeek-specific focus
)
```

### Mock Provider for Testing

```python
from xlibrary.ai import AIManager

# Mock provider for testing and development
mock_ai = AIManager(provider="mock", api_key="test-key", model="latest")

# Predictable responses for testing
response = mock_ai.request("Hello")
print(response.content)  # Consistent mock response

# Configure mock behavior
mock_ai = AIManager(
    provider="mock",
    api_key="test-key",
    mock_response_pattern="test",      # Customize mock responses
    mock_latency=0.1,                  # Simulate API latency
    mock_error_rate=0.0                # Simulate errors (0.0 = no errors)
)

# Test streaming with mock
for chunk in mock_ai.stream("Test streaming"):
    print(chunk.content, end="")

# Test error handling with mock
mock_ai_with_errors = AIManager(
    provider="mock",
    api_key="test-key",
    mock_error_rate=0.5                # 50% chance of error
)

try:
    response = mock_ai_with_errors.request("This might fail")
except Exception as e:
    print(f"Expected test error: {e}")
```

---

## Conversation Management

### Creating and Managing Conversations

```python
from xlibrary.ai import AIManager

ai = AIManager(provider="claude",
    api_key="your-api-key", model="current")

# Create a new conversation
conversation = ai.create_conversation()

# Basic conversation flow
conversation.add_message("What is machine learning?")
response1 = conversation.send()
print("AI:", response1.content)

conversation.add_message("Give me a simple example")
response2 = conversation.send()  # Knows context from previous exchange
print("AI:", response2.content)

# Check conversation history
print(f"Messages in conversation: {len(conversation.messages)}")
for i, message in enumerate(conversation.messages):
    role = "Human" if message.role == "user" else "AI"
    print(f"{i+1}. {role}: {message.content[:100]}...")
```

### Advanced Conversation Features

```python
# Conversation with system message
conversation = ai.create_conversation(
    system_message="You are a helpful Python programming tutor. Always provide code examples."
)

# Conversation with configuration
conversation = ai.create_conversation(
    max_tokens=2000,
    temperature=0.5,
    enable_reasoning=True
)

# Add messages with metadata
conversation.add_message(
    "Explain recursion in Python",
    metadata={
        "user_id": "user123",
        "session_id": "session456",
        "timestamp": "2025-01-15T10:30:00Z"
    }
)

response = conversation.send()

# Access conversation metadata
print("Conversation ID:", conversation.conversation_id)
print("Total messages:", conversation.message_count)
print("Total tokens used:", conversation.total_tokens)
```

### Conversation Persistence

```python
import json

# Save conversation state
conversation_data = conversation.to_dict()

# Serialize to JSON
with open("conversation.json", "w") as f:
    json.dump(conversation_data, f)

# Restore conversation
with open("conversation.json", "r") as f:
    saved_data = json.load(f)

# Create conversation from saved data
restored_conversation = ai.create_conversation_from_data(saved_data)

# Continue the restored conversation
restored_conversation.add_message("Continue where we left off")
response = restored_conversation.send()
```

### Multi-Turn Conversation Patterns

```python
def interactive_conversation():
    ai = AIManager(provider="claude",
    api_key="your-api-key", model="current")
    conversation = ai.create_conversation(
        system_message="You are a knowledgeable assistant. Keep responses concise but helpful."
    )

    print("Starting conversation (type 'exit' to end)")

    while True:
        user_input = input("\nYou: ")

        if user_input.lower() in ['exit', 'quit', 'bye']:
            break

        conversation.add_message(user_input)
        response = conversation.send()
        print(f"AI: {response.content}")

        # Show conversation stats
        if len(conversation.messages) % 10 == 0:  # Every 10 messages
            print(f"\n[Conversation stats: {len(conversation.messages)} messages, "
                  f"{conversation.total_tokens} tokens used]")

interactive_conversation()
```

### Conversation Branching

```python
# Create a conversation branch for different topics
main_conversation = ai.create_conversation()
main_conversation.add_message("Tell me about Python")
response = main_conversation.send()

# Branch 1: Focus on syntax
syntax_branch = main_conversation.branch()
syntax_branch.add_message("Focus on Python syntax details")
syntax_response = syntax_branch.send()

# Branch 2: Focus on applications
applications_branch = main_conversation.branch()
applications_branch.add_message("What are Python's main applications?")
applications_response = applications_branch.send()

# Both branches maintain the original context but diverge
print("Syntax branch:", syntax_response.content[:100])
print("Applications branch:", applications_response.content[:100])
```

---

## Streaming and Real-Time Features

### Basic Streaming

```python
from xlibrary.ai import AIManager

ai = AIManager(provider="claude",
    api_key="your-api-key", model="current")

# Simple streaming
print("AI Response: ", end="", flush=True)
for chunk in ai.stream("Write a short story about a robot"):
    print(chunk.content, end="", flush=True)
print()  # New line at the end
```

### Advanced Streaming with Callbacks

```python
def on_chunk_received(chunk):
    """Callback for each streaming chunk"""
    print(f"[{chunk.model}] {chunk.content}", end="", flush=True)

def on_stream_complete(final_response):
    """Callback when streaming is complete"""
    print(f"\n\nStream complete. Total tokens: {final_response.usage.get('total_tokens', 'N/A')}")

def on_stream_error(error):
    """Callback for streaming errors"""
    print(f"\nStreaming error: {error}")

# Stream with callbacks
ai.stream(
    "Explain the theory of relativity",
    on_chunk=on_chunk_received,
    on_complete=on_stream_complete,
    on_error=on_stream_error
)
```

### Async Streaming

```python
import asyncio
from xlibrary.ai import AIManager

async def async_streaming_example():
    ai = AIManager(provider="claude",
    api_key="your-api-key", model="current")

    # Async streaming
    async for chunk in ai.astream("Write a poem about technology"):
        print(chunk.content, end="", flush=True)
        await asyncio.sleep(0.01)  # Small delay for effect

# Run async streaming
asyncio.run(async_streaming_example())
```

### Conversation Streaming

```python
# Stream conversation responses
conversation = ai.create_conversation()
conversation.add_message("Tell me a long story about space exploration")

print("AI: ", end="", flush=True)
response = conversation.send_stream()

for chunk in response:
    print(chunk.content, end="", flush=True)

print()  # New line when complete
```

### Streaming with Buffer Management

```python
class StreamBuffer:
    def __init__(self, buffer_size=100):
        self.buffer = ""
        self.buffer_size = buffer_size

    def process_chunk(self, chunk):
        self.buffer += chunk.content

        # Process complete words/sentences
        if len(self.buffer) >= self.buffer_size or chunk.content.endswith(('.', '!', '?')):
            processed_text = self.buffer
            self.buffer = ""
            return processed_text

        return None

# Use buffer for streaming
buffer = StreamBuffer(50)
ai = AIManager(provider="claude", api_key="your-api-key")

for chunk in ai.stream("Write about artificial intelligence"):
    processed = buffer.process_chunk(chunk)
    if processed:
        print(f"Processed: {processed}")

# Process remaining buffer
if buffer.buffer:
    print(f"Final: {buffer.buffer}")
```

---

## File Attachments and Vision

### Image Analysis

```python
from xlibrary.ai import AIManager

ai = AIManager(provider="claude",
    api_key="your-api-key", model="current")  # Claude has excellent vision

# Single image analysis
response = ai.request(
    "Describe this image in detail. What objects, people, or text do you see?",
    attachments=["path/to/image.jpg"]
)

print(response.content)

# Multiple images comparison
response = ai.request(
    "Compare these two images and highlight the differences",
    attachments=["image1.jpg", "image2.jpg"]
)

# Image with specific questions
response = ai.request(
    "1. What is the main subject of this image?\n"
    "2. Are there any text elements visible?\n"
    "3. What is the overall mood or atmosphere?",
    attachments=["photo.png"],
    max_tokens=1000
)
```

### Document Processing

```python
# PDF document analysis
response = ai.request(
    "Summarize the key points from this document and extract any important data",
    attachments=["report.pdf"],
    max_tokens=2000
)

# Multiple document processing
response = ai.request(
    "Compare these three research papers and identify common themes",
    attachments=["paper1.pdf", "paper2.pdf", "paper3.pdf"],
    max_tokens=4000
)

# Structured document extraction
response = ai.request(
    "Extract the following information from this invoice:\n"
    "- Invoice number\n"
    "- Date\n"
    "- Total amount\n"
    "- Company name\n"
    "Return as JSON format",
    attachments=["invoice.pdf"]
)
```

### Advanced File Handling

```python
from pathlib import Path
import base64

# Custom file encoding
def encode_image(image_path):
    """Encode image to base64 for direct transmission"""
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

# Direct base64 image
image_b64 = encode_image("image.jpg")
response = ai.request(
    "Analyze this image",
    attachments=[{
        "type": "image",
        "data": image_b64,
        "format": "jpeg"
    }]
)

# Batch file processing
def process_image_directory(directory_path, query):
    """Process all images in a directory"""
    image_dir = Path(directory_path)
    image_files = list(image_dir.glob("*.{jpg,png,jpeg,gif}"))

    for image_file in image_files:
        print(f"\nProcessing {image_file.name}:")
        response = ai.request(
            query,
            attachments=[str(image_file)]
        )
        print(response.content)

# Process all images
process_image_directory("./images", "What is the main subject of this image?")
```

### Vision with Streaming

```python
# Stream vision analysis
response_stream = ai.stream(
    "Analyze this complex diagram and explain each component step by step",
    attachments=["complex_diagram.png"]
)

print("Analysis: ", end="", flush=True)
for chunk in response_stream:
    print(chunk.content, end="", flush=True)
print()
```

---

## Production Features

### Metrics and Monitoring

```python
from xlibrary.ai import AIManager, AIMetrics

# Enable metrics collection
ai = AIManager(
    provider="claude",
    api_key="your-api-key",
    api_key="your-api-key",
    model="current",
    enable_metrics=True
)

# Make some requests
for i in range(5):
    response = ai.request(f"Test request {i}")

# Get metrics summary
metrics = ai.get_metrics()
print("Metrics Summary:")
print(f"Total requests: {metrics.get_counter('requests_total')}")
print(f"Success rate: {metrics.get_success_rate():.2%}")
print(f"Average response time: {metrics.get_average_duration():.3f}s")
print(f"Total tokens used: {metrics.get_counter('tokens_total')}")
print(f"Estimated cost: ${metrics.get_total_cost():.4f}")

# Provider-specific metrics
claude_metrics = metrics.get_provider_metrics("claude")
print(f"Claude requests: {claude_metrics['requests']}")
print(f"Claude tokens: {claude_metrics['tokens']}")
```

### Health Monitoring

```python
from xlibrary.ai import AIManager, HealthChecker

# Enable health checks
ai = AIManager(
    provider="claude",
    api_key="your-api-key",
    model="current",
    enable_health_checks=True,
    health_check_interval=300  # Check every 5 minutes
)

# Manual health check
health = ai.check_health()
print(f"Provider health: {health.status}")
print(f"Response time: {health.response_time}ms")
print(f"Error rate: {health.error_rate:.2%}")

# Health check with callback
def on_health_change(provider, old_status, new_status):
    print(f"{provider} health changed: {old_status} -> {new_status}")

ai.set_health_callback(on_health_change)
```

### Rate Limiting

```python
from xlibrary.ai import AIManager, RateLimitConfig

# Configure rate limiting
rate_config = RateLimitConfig(
    requests_per_minute=60,    # 60 requests per minute
    tokens_per_minute=100000,  # 100k tokens per minute
    concurrent_requests=5      # Max 5 concurrent requests
)

ai = AIManager(
    provider="openai",
    model="gpt-4o",
    enable_rate_limiting=True,
    rate_limit_config=rate_config,
    provider_tier="pro"        # pro, team, enterprise
)

# Rate limiting is automatic
try:
    for i in range(100):
        response = ai.request(f"Request {i}")
        print(f"Processed request {i}")
except RateLimitError as e:
    print(f"Rate limited: {e.retry_after} seconds until retry")
```

### Structured Logging

```python
from xlibrary.ai import AIManager, LogConfig
import logging

# Configure logging
log_config = LogConfig(
    level=logging.INFO,
    format="structured",       # structured JSON logs
    include_request_data=True, # Log request details
    include_response_data=False, # Don't log response content (privacy)
    log_file="ai_operations.log"
)

ai = AIManager(
    provider="claude",
    api_key="your-api-key",
    model="current",
    enable_logging=True,
    log_config=log_config
)

# Requests are automatically logged
response = ai.request("Test message")

# Access logs programmatically
logs = ai.get_recent_logs(limit=10)
for log_entry in logs:
    print(f"{log_entry.timestamp}: {log_entry.event} - {log_entry.message}")
```

### Production Configuration Example

```python
from xlibrary.ai import AIManager, AIConfig, RateLimitConfig, LogConfig
import logging

# Complete production configuration
config = AIConfig(
    provider="claude",
    model="current",
    max_tokens=2048,
    temperature=0.5,           # More deterministic for production
    timeout=30.0,
    retries=3,
    enable_streaming=True,
    content_filtering=True     # Enable safety filters
)

rate_config = RateLimitConfig(
    requests_per_minute=300,   # Adjust based on your tier
    tokens_per_minute=500000,
    concurrent_requests=10
)

log_config = LogConfig(
    level=logging.INFO,
    format="structured",
    include_request_data=True,
    log_file="production_ai.log"
)

# Production-ready AI manager
production_ai = AIManager(
    provider="claude",
    api_key="your-api-key",
    config=config,
    enable_metrics=True,
    enable_health_checks=True,
    enable_rate_limiting=True,
    rate_limit_config=rate_config,
    enable_logging=True,
    log_config=log_config,
    provider_tier="pro"
)

# Use in production
try:
    response = production_ai.request("Production request")
    print(response.content)
except Exception as e:
    # Errors are automatically logged
    print(f"Production error: {e}")
```

---

## Testing and Debugging

### Comprehensive Testing Framework

```python
from xlibrary.ai import AIManager

# Built-in testing interface
ai = AIManager(provider="claude",
    api_key="your-api-key", model="current")

# Run built-in tests
test_results = ai.test(
    test_types=["basic", "streaming", "conversation"],
    include_real_api=True,     # Test against real API
    verbose=True
)

print(f"Tests passed: {test_results.passed}/{test_results.total}")

for test in test_results.failed_tests:
    print(f"Failed: {test.name} - {test.error}")
```

### Custom Test Suite

```python
import unittest
from xlibrary.ai import AIManager, MissingCredentialsError

class TestAIManager(unittest.TestCase):
    def setUp(self):
        """Set up test fixtures"""
        self.mock_ai = AIManager(provider="mock", api_key="test-key", model="latest")

        # Try to create real AI instance for integration tests
        try:
            self.real_ai = AIManager(provider="claude",
    api_key="your-api-key", model="current")
            self.has_real_api = True
        except MissingCredentialsError:
            self.has_real_api = False

    def test_basic_request(self):
        """Test basic request functionality"""
        response = self.mock_ai.request("Hello")
        self.assertIsNotNone(response)
        self.assertIsInstance(response.content, str)
        self.assertGreater(len(response.content), 0)

    def test_streaming(self):
        """Test streaming functionality"""
        chunks = list(self.mock_ai.stream("Test streaming"))
        self.assertGreater(len(chunks), 0)

        # Verify all chunks have content
        for chunk in chunks:
            self.assertIsInstance(chunk.content, str)

    def test_conversation(self):
        """Test conversation management"""
        conv = self.mock_ai.create_conversation()
        conv.add_message("Hello")
        response = conv.send()

        self.assertEqual(len(conv.messages), 2)  # User + AI message
        self.assertIsNotNone(response)

    @unittest.skipUnless(lambda self: self.has_real_api, "No real API credentials")
    def test_real_api_integration(self):
        """Integration test with real API"""
        response = self.real_ai.request("Say 'test successful'")
        self.assertIn("test successful", response.content.lower())

if __name__ == "__main__":
    unittest.main()
```

### Debug Mode and Troubleshooting

```python
from xlibrary.ai import AIManager
import logging

# Enable debug mode
logging.basicConfig(level=logging.DEBUG)

ai = AIManager(
    provider="claude",
    api_key="your-api-key",
    model="current",
    enable_logging=True,
    debug_mode=True            # Enable debug features
)

# Debug information is automatically logged
response = ai.request("Debug test message")

# Access debug information
debug_info = ai.get_debug_info()
print("Last request debug info:")
print(f"Provider: {debug_info.provider}")
print(f"Model: {debug_info.model}")
print(f"Request size: {debug_info.request_size} bytes")
print(f"Response size: {debug_info.response_size} bytes")
print(f"Duration: {debug_info.duration:.3f}s")
print(f"Tokens used: {debug_info.tokens_used}")
```

### Error Simulation for Testing

```python
# Simulate various error conditions for testing
mock_ai = AIManager(
    provider="mock",
    api_key="test-key",
    mock_error_scenarios={
        "rate_limit": 0.1,         # 10% chance of rate limit error
        "timeout": 0.05,           # 5% chance of timeout
        "authentication": 0.02,     # 2% chance of auth error
        "server_error": 0.03       # 3% chance of server error
    }
)

# Test error handling
for i in range(20):
    try:
        response = mock_ai.request(f"Test {i}")
        print(f"Success {i}: {response.content[:50]}...")
    except Exception as e:
        print(f"Expected error {i}: {type(e).__name__} - {e}")
```

---

## Performance Optimization

### Connection Pooling and Reuse

```python
from xlibrary.ai import AIManager

# Reuse AI manager instances for better performance
ai = AIManager(provider="claude",
    api_key="your-api-key", model="current")

# Good: Reuse instance
requests = ["Question 1", "Question 2", "Question 3"]
responses = []

for request in requests:
    response = ai.request(request)  # Reuses connections
    responses.append(response)

# Avoid: Creating new instances
# for request in requests:
#     ai = AIManager(provider="claude")  # Creates new connection each time
#     response = ai.request(request)
#     responses.append(response)
```

### Async and Concurrent Processing

```python
import asyncio
from xlibrary.ai import AIManager

async def process_requests_concurrently():
    ai = AIManager(provider="claude",
    api_key="your-api-key", model="current")

    requests = [
        "Explain machine learning",
        "What is quantum computing?",
        "Describe blockchain technology",
        "How does AI work?",
        "What is cloud computing?"
    ]

    # Process requests concurrently
    tasks = [ai.arequest(req) for req in requests]
    responses = await asyncio.gather(*tasks)

    for i, response in enumerate(responses):
        print(f"Response {i+1}: {response.content[:100]}...")

# Run async processing
asyncio.run(process_requests_concurrently())
```

### Caching and Optimization

```python
from functools import lru_cache
import hashlib

class OptimizedAIManager:
    def __init__(self, ai_manager):
        self.ai = ai_manager
        self.cache = {}

    def cached_request(self, message, cache_ttl=3600):
        """Request with caching support"""
        # Create cache key
        cache_key = hashlib.md5(
            f"{self.ai.provider}:{self.ai.model}:{message}".encode()
        ).hexdigest()

        # Check cache
        if cache_key in self.cache:
            cached_response, timestamp = self.cache[cache_key]
            if time.time() - timestamp < cache_ttl:
                return cached_response

        # Make request
        response = self.ai.request(message)

        # Cache response
        self.cache[cache_key] = (response, time.time())
        return response

# Use optimized manager
ai = AIManager(provider="claude",
    api_key="your-api-key", model="current")
optimized_ai = OptimizedAIManager(ai)

# Repeated requests use cache
response1 = optimized_ai.cached_request("What is Python?")  # API call
response2 = optimized_ai.cached_request("What is Python?")  # From cache
```

### Batch Processing

```python
def batch_process_requests(requests, batch_size=5, delay=1.0):
    """Process requests in batches to manage rate limits"""
    ai = AIManager(provider="claude",
    api_key="your-api-key", model="current")
    results = []

    for i in range(0, len(requests), batch_size):
        batch = requests[i:i + batch_size]
        batch_results = []

        print(f"Processing batch {i//batch_size + 1}: {len(batch)} requests")

        for request in batch:
            try:
                response = ai.request(request)
                batch_results.append({
                    "request": request,
                    "response": response.content,
                    "success": True
                })
            except Exception as e:
                batch_results.append({
                    "request": request,
                    "error": str(e),
                    "success": False
                })

        results.extend(batch_results)

        # Delay between batches
        if i + batch_size < len(requests):
            time.sleep(delay)

    return results

# Use batch processing
large_request_list = [f"Question {i}" for i in range(50)]
results = batch_process_requests(large_request_list, batch_size=10, delay=2.0)

success_count = sum(1 for r in results if r["success"])
print(f"Processed {success_count}/{len(results)} requests successfully")
```

---

## Integration Patterns

### Integration with Web Frameworks

#### Flask Integration
```python
from flask import Flask, request, jsonify, stream_template
from xlibrary.ai import AIManager

app = Flask(__name__)
ai = AIManager(provider="claude",
    api_key="your-api-key", model="current")

@app.route('/api/chat', methods=['POST'])
def chat():
    data = request.json
    message = data.get('message', '')

    try:
        response = ai.request(message)
        return jsonify({
            "response": response.content,
            "model": response.model,
            "provider": response.provider
        })
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/stream-chat', methods=['POST'])
def stream_chat():
    data = request.json
    message = data.get('message', '')

    def generate():
        try:
            for chunk in ai.stream(message):
                yield f"data: {chunk.content}\n\n"
        except Exception as e:
            yield f"data: [ERROR] {str(e)}\n\n"

    return app.response_class(generate(), mimetype='text/plain')

if __name__ == '__main__':
    app.run(debug=True)
```

#### FastAPI Integration
```python
from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from xlibrary.ai import AIManager

app = FastAPI()
ai = AIManager(provider="claude",
    api_key="your-api-key", model="current")

class ChatRequest(BaseModel):
    message: str
    stream: bool = False

class ChatResponse(BaseModel):
    response: str
    model: str
    provider: str

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    try:
        response = await ai.arequest(request.message)
        return ChatResponse(
            response=response.content,
            model=response.model,
            provider=response.provider
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/stream-chat")
async def stream_chat(request: ChatRequest):
    async def generate():
        try:
            async for chunk in ai.astream(request.message):
                yield f"data: {chunk.content}\n\n"
        except Exception as e:
            yield f"data: [ERROR] {str(e)}\n\n"

    return StreamingResponse(generate(), media_type="text/plain")
```

### Database Integration

```python
import sqlite3
from datetime import datetime
from xlibrary.ai import AIManager

class ChatDatabase:
    def __init__(self, db_path="chat.db"):
        self.db_path = db_path
        self.init_database()

    def init_database(self):
        """Initialize database schema"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute('''
            CREATE TABLE IF NOT EXISTS conversations (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                user_id TEXT NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')

        cursor.execute('''
            CREATE TABLE IF NOT EXISTS messages (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                conversation_id INTEGER,
                role TEXT NOT NULL,
                content TEXT NOT NULL,
                model TEXT,
                provider TEXT,
                tokens_used INTEGER,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (conversation_id) REFERENCES conversations (id)
            )
        ''')

        conn.commit()
        conn.close()

    def create_conversation(self, user_id):
        """Create new conversation"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute(
            "INSERT INTO conversations (user_id) VALUES (?)",
            (user_id,)
        )

        conversation_id = cursor.lastrowid
        conn.commit()
        conn.close()
        return conversation_id

    def add_message(self, conversation_id, role, content, model=None, provider=None, tokens_used=None):
        """Add message to conversation"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute(
            """
            INSERT INTO messages
            (conversation_id, role, content, model, provider, tokens_used)
            VALUES (?, ?, ?, ?, ?, ?)
            """,
            (conversation_id, role, content, model, provider, tokens_used)
        )

        conn.commit()
        conn.close()

class PersistentAIChat:
    def __init__(self):
        self.ai = AIManager(provider="claude",
    api_key="your-api-key", model="current")
        self.db = ChatDatabase()

    def start_conversation(self, user_id):
        """Start new conversation"""
        return self.db.create_conversation(user_id)

    def send_message(self, conversation_id, user_id, message):
        """Send message and get AI response"""
        # Save user message
        self.db.add_message(conversation_id, "user", message)

        # Get AI response
        response = self.ai.request(message)

        # Save AI response
        self.db.add_message(
            conversation_id,
            "assistant",
            response.content,
            response.model,
            response.provider,
            response.usage.get("total_tokens") if response.usage else None
        )

        return response

# Use persistent chat
chat = PersistentAIChat()
conv_id = chat.start_conversation("user123")
response = chat.send_message(conv_id, "user123", "Hello, AI!")
```

### Integration with Other xlibrary Pillars

```python
from xlibrary.ai import AIManager
from xlibrary.config import ConfigManager
from xlibrary.download import DownloadManager
from xlibrary.media import MediaManager

# Multi-pillar integration example
class SmartContentProcessor:
    def __init__(self):
        # Load configuration
        self.config = ConfigManager("app.toml")

        # Initialize AI with config
        self.ai = AIManager(
            provider=self.config.get("ai.provider", "claude"),
            model=self.config.get("ai.model", "current"),
            api_key=self.config.get("ai.api_key")
        )

        # Initialize other pillars
        self.downloader = DownloadManager()
        self.media = MediaManager()

    def process_video_url(self, video_url, analysis_query):
        """Download video, extract frames, and analyze with AI"""

        # Download video
        print("Downloading video...")
        download_result = self.downloader.download(video_url)

        if not download_result.success:
            return {"error": "Failed to download video"}

        # Extract key frame
        print("Extracting frame...")
        frame_path = self.media.extract_frame(
            download_result.local_path,
            timestamp="00:01:00"  # Extract frame at 1 minute
        )

        # Analyze frame with AI
        print("Analyzing with AI...")
        response = self.ai.request(
            analysis_query,
            attachments=[frame_path]
        )

        return {
            "video_path": download_result.local_path,
            "frame_path": frame_path,
            "analysis": response.content,
            "model_used": response.model
        }

# Use integrated processor
processor = SmartContentProcessor()
result = processor.process_video_url(
    "https://example.com/video.mp4",
    "Describe what's happening in this video frame"
)

print("Analysis:", result["analysis"])
```

---

## Next Steps and Advanced Topics

### Extending the AI Manager

For advanced use cases, you can extend the AI Manager:

```python
from xlibrary.ai import AIManager, BaseAIProvider

class CustomAIManager(AIManager):
    """Extended AI Manager with custom features"""

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.custom_cache = {}

    def intelligent_request(self, message, context=None):
        """Request with intelligent context management"""
        if context:
            enhanced_message = f"Context: {context}\n\nQuery: {message}"
        else:
            enhanced_message = message

        response = self.request(enhanced_message)

        # Custom post-processing
        response.content = self._post_process_response(response.content)
        return response

    def _post_process_response(self, content):
        """Custom response post-processing"""
        # Add custom logic here
        return content

# Use extended manager
custom_ai = CustomAIManager(provider="claude", api_key="your-api-key", model="current")
response = custom_ai.intelligent_request(
    "Explain quantum computing",
    context="The user is a beginner programmer"
)
```

### Performance Monitoring Dashboard

Create a simple monitoring dashboard:

```python
import time
from collections import defaultdict
from xlibrary.ai import AIManager

class AIPerformanceMonitor:
    def __init__(self):
        self.metrics = defaultdict(list)
        self.start_time = time.time()

    def record_request(self, provider, model, duration, tokens, success):
        """Record request metrics"""
        self.metrics['requests'].append({
            'timestamp': time.time(),
            'provider': provider,
            'model': model,
            'duration': duration,
            'tokens': tokens,
            'success': success
        })

    def get_dashboard_data(self):
        """Get dashboard metrics"""
        requests = self.metrics['requests']
        if not requests:
            return {"message": "No requests recorded"}

        # Calculate statistics
        total_requests = len(requests)
        successful_requests = sum(1 for r in requests if r['success'])
        avg_duration = sum(r['duration'] for r in requests) / total_requests
        total_tokens = sum(r['tokens'] or 0 for r in requests)

        # Provider breakdown
        provider_stats = defaultdict(int)
        for request in requests:
            provider_stats[request['provider']] += 1

        return {
            'total_requests': total_requests,
            'success_rate': (successful_requests / total_requests) * 100,
            'average_duration': avg_duration,
            'total_tokens': total_tokens,
            'provider_breakdown': dict(provider_stats),
            'uptime': time.time() - self.start_time
        }

# Use monitor
monitor = AIPerformanceMonitor()
ai = AIManager(provider="claude",
    api_key="your-api-key", model="current")

# Simulate requests with monitoring
for i in range(10):
    start_time = time.time()
    try:
        response = ai.request(f"Test request {i}")
        duration = time.time() - start_time
        tokens = response.usage.get('total_tokens', 0) if response.usage else 0

        monitor.record_request(
            provider=ai.provider,
            model=response.model,
            duration=duration,
            tokens=tokens,
            success=True
        )
    except Exception as e:
        duration = time.time() - start_time
        monitor.record_request(
            provider=ai.provider,
            model=ai.model,
            duration=duration,
            tokens=0,
            success=False
        )

# Display dashboard
dashboard = monitor.get_dashboard_data()
print("AI Performance Dashboard:")
print(f"Total Requests: {dashboard['total_requests']}")
print(f"Success Rate: {dashboard['success_rate']:.1f}%")
print(f"Average Duration: {dashboard['average_duration']:.3f}s")
print(f"Total Tokens: {dashboard['total_tokens']}")
print(f"Provider Breakdown: {dashboard['provider_breakdown']}")
```

---

**Congratulations! You now have comprehensive knowledge of the xlibrary AI Manager.**

## What's Next?

- **[Chapter 2: Config Manager](02.01%20Chapter%202%20-%20Config%20Manager%20-%20Design%20-%20Overview.md)** - Master configuration management
- **[Chapter 3: Download Manager](03.01%20Chapter%203%20-%20Download%20Manager%20-%20Design%20-%20Overview.md)** - Advanced download capabilities
- **[Other Pillars](00.00%20Chapter%200%20-%20Introduction.md#documentation-structure)** - Explore the full xlibrary ecosystem

**You're now ready to build sophisticated AI applications with xlibrary!** ðŸš€